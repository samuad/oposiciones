
\chapter{Autocorrelaci\'on. Naturaleza y causas de la autocorrelaci\'on. Consecuencias
de la autocorrelaci\'on. Contrastes de autocorrelaci\'on. Estimaci\'on de
modelos con autocorrelaci\'on. Predicci\'on.}


\section{Introducci\'on.}

En este caso examinamos modelos econom\'etricos en los que la matriz
de covarianzas del t\'ermino de error no es escalar porque presenta
elementos distintos de cero fuera de la diagonal principal. Esto proviene
del hecho de que el t\'ermino de error del modelo guarda correlaci\'on
consigo mismo para distintas observaciones, por eso se llama autocorrelaci\'on
al fen\'omeno. El objetivo es establecer una serie de contrastes que
nos permitan comprobar la hip\'otesis nula de ausencia de autocorrelaci\'on
y, en caso de rechazar esta hip\'otesis, estimar el modelo econom\'etrico
bajo una determinada estructura de autocorrelaci\'on.


\section{Naturaleza y causas de la autocorrelaci\'on.}

Existe autocorrelaci\'on cuando el t\'ermino de error del modelo est\'a
correlacionado consigo mismo, es decir, $E\left(u_{i}u_{j}\right)\neq0$.
No es necesario que esta autocorrelaci\'on se produzca en toda la muestra,
basta en algunos casos. La correlaci\'on no tiene por que producirse
entre valores consecutivos. La correlaci\'on puede producirse por diversas
causas:
\begin{itemize}
\item Existencia de ciclos o tendencias: Si la variable end\'ogena del modelo
presenta ciclos y \'estos no son bien explicados por las variables ex\'ogenas
del modelo, el t\'ermino de error presentar\'a autocorrelaci\'on, ya que
los errores grandes tender\'an a estar agrupados. Igualmente, si la
variable presenta una tendencia no bien explicada por las variables
explicativas, los t\'erminos de error ser\'an negativos al principio,
ir\'an disminuyendo y se har\'an positivos al final.
\item Variables omitidas: Si el verdadero modelo que explica el comportamiento
de la variable end\'ogena es:
\[
y_{i}=\beta_{1}+\beta_{2}x_{2i}+\beta_{3}x_{3i}+u_{i}
\]
pero se estima el modelo $y_{i}=\beta_{1}+\beta_{2}x_{2i}+v_{i}$,
entonces el t\'ermino de error es $v_{i}=u_{i}+\beta_{3}x_{3i}$. Si
la variable $x_{3}$ est\'a correlacionada consigo misma (tendencias,
ciclos, etc...), entonces $v_{t}$ presentar\'a correlaci\'on. En este
caso, la ausencia de variables en el modelo presenta otros problemas
aparte de la correlaci\'on, por lo que se deber\'ian intentar identificar
si se sospecha de su presencia.
\item Relaciones no lineales: Si la relaci\'on es no lineal, por ejemplo:
$y_{i}=\beta_{1}+\beta_{2}x_{i}+\beta_{3}x_{i}^{2}+u_{i}$. Si este
modelo se especifica de forma lineal, nos encontraremos con una racha
de residuos negativos, seguida de una racha de residuos positivos
para acabar con otra racha de residuos negativos, lo que generar\'a
autocorrelaci\'on del t\'ermino de error.
\item Relaciones din\'amicas: La mayor\'ia de relaciones entre variables econ\'omicas
se extienden a m\'as de un per\'iodo. As\'i, la relaci\'on entre la inflaci\'on
y el crecimiento de la oferta monetaria es del tipo $\pi_{t}=\beta_{1}+\beta_{2}m_{t}+\beta_{3}\pi_{t-1}+u_{t}$.
Si omitimos el retardo de la variable end\'ogena, el t\'ermino de error
del modelo incorporar\'a dicha variable, mostrando autocorrelaci\'on.
\end{itemize}

\section{Consecuencias de la autocorrelaci\'on.}

Si un modelo lineal presenta autocorrelaci\'on, su estimador de m\'inimos
cuadrados ordinarios, si bien es insesgado, ya no es el estimador
lineal insesgado de m\'inima varianza, dicha propiedad corresponde al
estimador de m\'inimos cuadrados generalizados. Este estimador se puede
obtener de dos formas: premultiplicando las matrices de observaciones
de las variables por la matriz $\boldsymbol{V}^{-1}$, siendo $\boldsymbol{V}$
una matriz tal que la matriz de covarianzas del t\'ermino de error del
modelo se descompone como$\boldsymbol{\Sigma}=\boldsymbol{V}\boldsymbol{V}^{\prime}$,
o resolviendo el sistema de ecuaciones normales $\hat{\boldsymbol{\beta}}_{MCG}=\left(\boldsymbol{X}^{\prime}\boldsymbol{\Sigma}^{-1}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\boldsymbol{\Sigma}^{-1}\boldsymbol{y}$.
Si a pesar de todo se utiliza el estimador MCO, hay que recordar que
su matriz de covarianzas es $\sigma_{u}^{2}\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\boldsymbol{\Sigma}\boldsymbol{X}\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}$.

Si no tenemos confianza en el modelo de autocorrelaci\'on y tememos
que una mala especificaci\'on introduzca sesgos en el estimador MCG
puede ser interesante obtener el estimador MCO para comparar. En estos
casos, Newey y West han propuesto utilizar como estimador de $\dfrac{1}{N}\boldsymbol{X}^{\prime}\boldsymbol{\Sigma}\boldsymbol{X}$
la matriz 
\[
\boldsymbol{S}=\dfrac{1}{N}\sum_{j=0}^{L}\sum_{i=j+1}^{N}w_{j}\hat{u}_{i}\hat{u}_{i-j}\left[\boldsymbol{x}_{i}\boldsymbol{x}_{i-j}^{\prime}+\boldsymbol{x}_{i-j}\boldsymbol{x}_{i}^{\prime}\right]
\]
con $w_{j}=1-\dfrac{j}{L+1}$ siendo $L$ el orden m\'aximo de autocorrelaci\'on
del t\'ermino de error, que no siempre es f\'acil de determinar.


\section{Contrastes de autocorrelaci\'on.}


\subsection{El contraste de Durbin-Watson.}

Contrasta la hip\'otesis nula de ausencia de autocorrelaci\'on contra
la hip\'otesis alternativa de autocorrelaci\'on de primer orden, del tipo
$u_{i}=\rho u_{i-1}+\varepsilon_{i}$. Se utiliza el estad\'istico de
Durbin-Watson: 
\[
d=\dfrac{\sum_{i=2}^{N}\left(\hat{u}_{i}-\hat{u}_{i-1}\right)^{2}}{\sum_{i=2}^{N}\hat{u}_{i}^{2}}
\]


La interpretaci\'on de este estad\'istico es la siguiente: si la correlaci\'on
es positiva, valores positivos del error tender\'an a estar seguidos
de valores positivos del mismos, y valores negativos tender\'an a estar
seguidos de valores negativos. Adem\'as, dado que el estimador MCO es
insesgado, los $\hat{u}_{i}$ ser\'an estimadores insesgados (aunque
ineficientes) de $u_{i}$. Por tanto, las diferencias $\hat{u}_{i}-\hat{u}_{i-1}$
tender\'an a ser menores en valor absoluto que $\hat{u}_{i}$ y por
tanto, $\left(\hat{u}_{i}-\hat{u}_{i-1}\right)^{2}<\hat{u}_{i}^{2}$
y el numerador de $d$ tender\'a a ser peque�o en comparaci\'on con el
denominador.

Si el coeficiente de correlaci\'on fuera negativo, tendr\'iamos tendencia
a tener valores positivos seguidos de valores negativos y viceversa
y por tanto el valor absoluto de $\hat{u}_{i}-\hat{u}_{i-1}$ tender\'a
a ser mayor que $\hat{u}_{i}$ y el estad\'istico tender\'a a tomar valores
grandes.

Desarrollando el estad\'istico, 
\[
d=\dfrac{\sum_{i=2}^{N}\hat{u}_{i}^{2}+\sum_{i=2}^{N}\hat{u}_{i-1}^{2}-2\sum_{i=2}^{N}\hat{u}_{i}\hat{u}_{i-1}}{\sum_{i=2}^{N}\hat{u}_{i}^{2}}
\]


Si el n\'umero de observaciones es suficientemente grande, $\sum_{i=2}^{N}\hat{u}_{i}^{2}\approx\sum_{i=2}^{N}\hat{u}_{i-1}^{2}$
y entonces $d\approx2\left(1-\hat{\rho}\right)$, ya que $\hat{\rho}=\dfrac{\sum_{i=2}^{N}\hat{u}_{i}\hat{u}_{i-1}}{\sum_{i=2}^{N}\hat{u}_{i}^{2}}$.
Como para que el t\'ermino de error no sea divergente se tiene que producir
que $\rho\in\left[-1;1\right]$, $d$ estar\'a entre 0 y 4, con valores
pr\'oximos a cero cuando exista autocorrelaci\'on positiva de primero
orden y valores pr\'oximos a 4 cuando la correlaci\'on sea negativa. Cuando
no exista autocorrelaci\'on del t\'ermino de error, el valor de $d$ estar\'a
pr\'oximo a 2.

Dado que los residuos dependen de la matriz $\boldsymbol{M}$, que
a su vez depende de de la matriz de observaciones, $\boldsymbol{X}$,
su distribuci\'on, su matriz de covarianzas, y por tanto la distribuci\'on
del estad\'istico cambian con cada matriz de observaciones y no se pueden
tabular. Hay tablas de las cotas superior e inferior de $d$ para
los niveles de significaci\'on sobre el conjunto de todas sus posibles
distribuciones. Si se quiere contrastar la hip\'otesis nula de ausencia
de autocorrelaci\'on contra la hip\'otesis alternativa de autocorrelaci\'on
positiva de primer orden y se obtiene un valor de $d$ por debajo
de la cota inferior, podemos afirmar que se rechaza la hip\'otesis nula,
sean cuales sean los valores de la matriz $\boldsymbol{X}$. Si el
valor de $d$ supera la cota superior, concluiremos que no puede rechazarse
la hip\'otesis nula. Si el valor se sit\'ua entre estas dos cotas, no
se puede tomar ninguna decisi\'on.

Si se quiere contrastar la hip\'otesis nula de ausencia de autocorrelaci\'on
contra la hip\'otesis alternativa de autocorrelaci\'on negativa de primer
orden, se lleva a cabo el mismo proceso, pero para el estad\'istico
$4-d$.

El problema de este contraste es que las cotas que se utilizan est\'an
obtenidas entre todas las distribuciones posibles de $d$, y por tanto
son demasiado estrictas, dejando muchos casos de indefinici\'on.

Las cotas obtenidas por Durbin y Watson suponen que hay un t\'ermino
independiente en el modelo, y que todas las variables explicativas
son deterministas. Por tanto, este supuesto no puede mantenerse si
se utilizan retardos de la variable dependiente como variables explicativas.
En esos casos este procedimiento produce estimaciones sesgadas del
par\'ametro $\rho$, y el sesgo disminuir\'a el valor absoluto del estimador,
lo que aumenta el riesgo de aceptaci\'on de la hip\'otesis nula aunque
se presente autocorrelaci\'on.


\subsection{Contraste de Wallis.}

Es una extensi\'on del contraste de Durbin-Watson para correlaciones
de cuarto orden, que son habituales en datos trimestrales. En este
caso, la especificaci\'on es: $u_{t}=\rho_{4}u_{t-4}+\varepsilon_{t}$.
Con el fin de contrastar la hip\'otesis nula Wallis propone un estad\'istico
de Durbin-Watson modificado: 
\[
d_{4}=\dfrac{\sum_{i=5}^{N}\left(\hat{u}_{i}-\hat{u}_{i-4}\right)^{2}}{\sum_{i=1}^{N}\hat{u}_{i}^{2}}
\]


De este estad\'istico se conocen l\'imites superiores e inferiores para
el caso de una matriz $\boldsymbol{X}$ no estoc\'astica, para modelos
con t\'ermino independiente y sin t\'ermino independiente.


\subsection{Contrastes de Durbin.}

EL contraste de Durbin-Watson exige que la matriz de observaciones
de las variables ex\'ogenas sea no estoc\'astica, lo que no se cumple
en el caso de que los regresores contengan retardos de la variable
dependiente. Para el caso general Durbin desarroll\'o una prueba asint\'otica
para muestras grandes. Se sigue contrastando contra la autocorrelaci\'on
de primer orden, y en \'el es necesario especificar el conjunto completo
de regresores. As\'i, si tenemos el modelo: 
\[
\begin{array}{c}
y_{t}=\beta_{1}+\beta_{2}y_{t-1}+\cdots+\beta_{r}y_{t-r}+\beta_{r+1}x_{1t}+\cdots+\beta_{r+s}x_{st}+u_{t}\\
u_{t}=\rho u_{t-1}+\varepsilon_{t}
\end{array}
\]
 con $\varepsilon_{t}\sim N\left(0,\sigma_{\varepsilon}^{2}\boldsymbol{I}\right)$.
El resultado obtenido por Durbin es que bajo la hip\'otesis nula, $H_{0}:\rho=0$
el estad\'istico 
\[
h=\hat{\rho}\sqrt{\dfrac{N}{1-NVar\left(\hat{\beta}_{2}\right)}}\overset{L}{\rightarrow}N\left(0,1\right)
\]


El procedimiento del contraste ser\'ia como sigue:
\begin{enumerate}
\item Ajustar la regresi\'on por MCO suponiendo ausencia de heteroscedasticidad,
y obtener el estimador de $Var\left(\hat{\beta}_{2}\right)$.
\item Partiendo de los residuos, calcular $\hat{\rho}$.
\item Calcular $h$, y si el valor de $N\left(0,1\right)$ al nivel de significaci\'on
elegido es menor, rechazar la hip\'otesis nula para ese valor de significaci\'on
(en el caso de una significaci\'on del $5\%$, el valor de $N\left(0,1\right)$
para una probabilidad del $95\%$ es de $1,645$.
\item Si la $h$ es negativa, se puede utilizar un contraste similar para
autocorrelaci\'on negativa.
\end{enumerate}
Si $NVar\left(\hat{\beta}_{2}\right)\geq1$ el contraste falla. Para
esos casos Durbin encontr\'o un procedimiento asint\'otico equivalente:
\begin{enumerate}
\item Estimar la regresi\'on MCO y obtener los residuos.
\item Estimar la regresi\'on MCO de $\hat{u}_{t}$ sobre $\hat{u}_{t-1},y_{t-1},\ldots,y_{t-r},x_{1t},\ldots,x_{st}$.
\item Si el coeficiente de $\hat{u}_{t-1}$ es significativamente distinto
de cero utilizando el contraste de significaci\'on habitual, se rechaza
la hip\'otesis nula.
\end{enumerate}
Durbin indica que este procedimiento puede utilizarse tambi\'en para
contrastar perturbaciones de orden mayor que uno, a�adiendo retardos
del t\'ermino de error a la regresi\'on.


\subsection{Contraste de Breusch y Godfrey.}

Para llevar a cabo contrastes que supongan una estructura del t\'ermino
error m\'as generales que autorregresi\'on de primer orden, generalizando
este podemos considerar el siguiente estad\'istico: 
\[
r_{k}=\dfrac{\sum_{i=1}^{N}\hat{u}_{i}\hat{u}_{i-k}}{\sum_{i=1}^{N}\hat{u}_{i}^{2}}
\]
 el caso con $k=1$ ser\'ia la estimaci\'on MCO del par\'ametro $\rho$
en el caso anterior. Los dem\'as estad\'isticos ser\'an o no significativos
dependiendo de la estructura de la autocorrelaci\'on del t\'ermino de
error. Esta estructura puede ser del tipo $u_{t}=\rho_{1}u_{t-1}+\rho_{2}u_{t-2}+\cdots+\rho_{p}u_{t-p}+\varepsilon_{t}$.
El contraste introducido simult\'aneamente por Breusch y Godfrey considera
este tipo de correlaci\'on como hip\'otesis alternativa.
\begin{enumerate}
\item Estimar el modelo por MCO y obtener los residuos. El modelo puede
incluir retardos de la variable end\'ogena sin que afecte a la validez
del contraste.
\item Estimar una regresi\'on de los residuos sobre $p$ retardos de los mismos
as\'i como sobre las variables explicativas del modelo original. El
n\'umero de retardos debe coincidir con el n\'umero de estad\'isticos $r_{k}$
cuya significaci\'on conjunta se quiere contrastar. Obtener el valor
$R^{2}$para esta regresi\'on.
\item Comparar $NR^{2}$ con la tabla de una distribuci\'on chi-cuadrado con
$p$ grados de libertad y rechazar la hip\'otesis nula de no autocorrelaci\'on
si $NR^{2}$ es superior al valor de las tablas.
\end{enumerate}
Dado que sabemos que los residuos son ortogonales a las variables
explicativas, si se cumple la hip\'otesis nula de ausencia de correlaci\'on
el valor de $R^{2}$ debe tender a cero mucho m\'as r\'apido de lo que
aumenta el tama�o de la muestra. La validez estricta del contraste
se reduce al caso en el que el tama�o muestral es infinito.

Este contraste es asint\'oticamente equivalente a la segunda versi\'on
del contraste de Durbin.


\section{Estimaci\'on de modelos con autocorrelaci\'on.}

SI el modelo presenta autocorrelaci\'on de los t\'erminos de error, los
valores fuera de la diagonal de la matriz de covarianzas ser\'an distintos
de cero, por lo que dicha matriz puede contener hasta $\dfrac{N\left(N+1\right)}{2}$
par\'ametros distintos a estimar. Claramente si no tenemos ninguna informaci\'on
sobre la forma de la autocorrelaci\'on es una tarea sin soluci\'on. Supongamos,
por ejemplo, que el t\'ermino de error sigue una autocorrelaci\'on autorregresiva
de primer orden: $u_{t}=\rho u_{t-1}+\varepsilon_{t}$, donde $\left|\rho\right|<1$
es un par\'ametro desconocido, y $\varepsilon_{t}$ es una variable
aleatoria con $E\left(\varepsilon_{t}\right)=0$, $Var\left(\varepsilon_{t}\right)=\sigma_{\varepsilon}^{2}$
y $Cov\left(\varepsilon_{t},\varepsilon_{s}\right)=0$ si $t\neq s$.
Como $\left|\rho\right|<1$, la perturbaci\'on s\'olo depende de $\varepsilon_{t}$
y de sus valores pasados. Adem\'as, se puede demostrar que $u_{t}=\sum_{i=0}^{\infty}\rho^{i}\varepsilon_{t-i}$,
y por tanto $Var\left(u_{t}\right)=\sigma_{u}^{2}=\dfrac{\sigma_{\varepsilon}^{2}}{1-\rho^{2}}$;
$Cov\left(u_{t},u_{s}\right)=\sigma_{ts}=\rho^{s-t}\sigma_{u}^{2}$,
con $s>t$. 
\[
\begin{array}{c}
Cov\left(u_{t-1},u_{t}\right)=\sigma_{t-1,t}=E\left(u_{t-1}u_{t}\right)=E\left[u_{t-1}\left(\rho u_{t-1}+\varepsilon_{t}\right)\right]=\rho E\left(u_{t-1}^{2}\right)+E\left(u_{t-1}\varepsilon_{t}\right)=\rho\sigma_{u}^{2}+0\\
Cov\left(u_{t-2},u_{t}\right)=\sigma_{t-2,t}=E\left(u_{t-2}u_{t}\right)=E\left[u_{t-2}\left(\rho u_{t-1}+\varepsilon_{t}\right)\right]=\rho E\left(u_{t-2}u_{t-1}\right)+E\left(u_{t-2}\varepsilon_{t}\right)=\rho^{2}\sigma_{u}^{2}+0\\
\vdots\\
Cov\left(u_{t-s},u_{t}\right)=\sigma_{t-s,t}=E\left(u_{t-s}u_{t}\right)=E\left[u_{t-s}\left(\rho u_{t-1}+\varepsilon_{t}\right)\right]=\rho E\left(u_{t-s}u_{t-1}\right)+E\left(u_{t-s}\varepsilon_{t}\right)=\rho^{s}\sigma_{u}^{2}+0
\end{array}
\]
y por tanto: 
\[
Cov\left(\boldsymbol{u}\right)=\sigma_{u}^{2}\boldsymbol{\Sigma}=\left(\begin{array}{ccccc}
1 & \rho & \rho^{2} & \cdots & \rho^{N-1}\\
\rho & 1 & \text{\ensuremath{\rho}} & \cdots & \rho^{N-2}\\
\rho^{2} & \rho & 1 & \cdots & \rho^{N-3}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
\rho^{N-1} & \rho^{N-2} & \rho^{N-3} & \cdots & 1
\end{array}\right)
\]
 que s\'olo tiene dos par\'ametros desconocidos, $\rho$ y $\sigma_{u}^{2}$.
Dado que son desconocidos, ser\'a necesario estimarlos para obtener
una estimaci\'on de la matriz $\boldsymbol{\Sigma}$, y poder estimar
el modelo mediante el estimador MCG. No obstante, hay que remarcar
que el hecho de utilizar una estimaci\'on de $\boldsymbol{\Sigma}$
hace que este estimador no podamos afirmar que es el de m\'inima varianza,
esto depender\'a de las propiedades del estimador de $\boldsymbol{\Sigma}$,
$\hat{\boldsymbol{\Sigma}}$. Podr\'iamos estimar el modelo mediante
MCO, y posteriormente estimar de nuevo por MCO la regresi\'on de $\hat{u}_{t}$
sobre $\hat{u}_{t-1}$, y de aqu\'i obtener los estimadores $\hat{\rho}$,
$\hat{\sigma}_{\varepsilon}^{2}$ y $\hat{\sigma}_{u}^{2}=\dfrac{\hat{\sigma}_{\varepsilon}^{2}}{1-\hat{\rho}^{2}}$.


\subsection{Estimaci\'on mediante transformaci\'on de variables.}

Tenemos nuestro modelo: 
\[
\begin{array}{c}
y_{t}=\boldsymbol{x}_{t}^{\prime}\boldsymbol{\beta}+u_{t}\\
u_{t}=\rho u_{t-1}+\varepsilon_{t}
\end{array}
\]


El modelo para el instante $t-1$ es: $y_{t-1}=\boldsymbol{x}_{t-1}^{\prime}\boldsymbol{\beta}+u_{t-1}$.

Y haciendo: $y_{t}-\rho y_{t-1}=\boldsymbol{x}_{t}^{\prime}\boldsymbol{\beta}-\rho\left(\boldsymbol{x}_{t-1}^{\prime}\boldsymbol{\beta}\right)+\varepsilon_{t}$,
y definiendo como nuevas variables $y_{t}^{*}=y_{t}-\rho y_{t-1}$,
$x_{it}^{*}=x_{it}-\rho x_{it-1}$ el modelo se convierte en $y_{t}^{*}=\boldsymbol{x}_{t}^{*\prime}\boldsymbol{\beta}+\varepsilon_{t}$,
que no tiene heteroscedasticidad, y por tanto su estimador MCO coincide
con su estimador MCG. Esta transformaci\'on ignora la primera observaci\'on
muestral, lo que produce una ligera p\'erdida de eficiencia, irrelevante
si la muestra es grande.

Podemos estimar el par\'ametro $\rho$ como hemos visto antes y efectuar
la transformaci\'on de variables con esta estimaci\'on.

Aplicando este m\'etodo es posible que no desaparezcan las autocorrelaciones
en los residuos, bien debidas a que provienen de una estimaci\'on de
$\rho$ que adem\'as es ineficiente, bien porque la estructura de la
autocorrelaci\'on sea m\'as compleja. En este \'ultimo caso existe la posibilidad
de iterar el m\'etodo.

La transformaci\'on que se aplica equivale a descomponer la matriz de
covarianzas y aplicarla a las variables del modelo para obtener t\'erminos
de error esf\'ericos.


\subsection{Procedimiento de Cochrane-Orcutt.}

Aplica la anterior transformaci\'on de modo iterativo:
\begin{itemize}
\item Estimar el modelo de regresi\'on mediante MCO, ignorando la presencia
de autocorrelaci\'on.
\item Utilizar los residuos MCO para estimar el par\'ametro $\rho$, o bien
mediante una regresi\'on de $u_{t}$ sobre $u_{t-1}$ o bien mediante
el estad\'istico de Durbin-Watson.
\item Utilizar la estimaci\'on de $\rho$ para obtener las variables cuasidiferenciadas.
\item Estimar el modelo con las variables cuasidiferenciadas por MCO, obteniendo
la estimaci\'on de los coeficientes.
\item Utilizar esta estimaci\'on para generar un nuevo conjunto de residuos.
Utilizar esto residuos para obtener una nueva estimaci\'on de $\rho$.
\item Repetir hasta alcanzar el grado de convergencia deseado.
\end{itemize}
Para evitar que el procedimiento converja a un m\'inimo local y no global,
se recomienda crear una red de b\'usqueda o partici\'on del espacio param\'etrico
y evaluar las suma de los cuadrados de los residuos en los nodos de
esa red, para tomar el valor de $\rho$ que minimice los mismos.

Este procedimiento de estimaci\'on puede extenderse al caso de autocorrelaci\'on
de orden superior a uno. Por ejemplo, si se tiene el modelo con autocorrelaci\'on
$u_{t}=\rho_{1}u_{t-1}+\rho_{2}u_{t-2}+\varepsilon_{t}$, las dos
ecuaciones a iterar ser\'ian: 
\[
\begin{array}{c}
y_{t}-\rho_{1}y_{t-1}-\rho_{2}y_{t-2}=\left(\boldsymbol{x}_{t}-\rho_{1}\boldsymbol{x}_{t-1}-\rho_{2}\boldsymbol{x}_{t-2}\right)^{\prime}\boldsymbol{\beta}+\varepsilon_{t}\\
\hat{u}_{t}=\rho_{1}\hat{u}_{t-1}+\rho_{2}\hat{u}_{t-2}+\varepsilon_{t}
\end{array}
\]



\section{Predicci\'on.}

Una vez hemos estimado el modelo transformado la forma de hacer predicciones
a partir de ese modelo. Veamos c\'omo se har\'ia para una autocorrelaci\'on
con un retardo. La especificaci\'on del modelo ser\'ia:

\[
\begin{array}{c}
y_{t}=\boldsymbol{x}_{t}^{\prime}\boldsymbol{\beta}+u_{t}\\
u_{t}=\rho u_{t-1}+\varepsilon_{t}
\end{array}
\]
 con $\left|\rho\right|\leq1$ y $Var(\varepsilon_{t})=\sigma_{\varepsilon}^{2}\boldsymbol{I}$.
Y haciendo: $y_{t}-\rho y_{t-1}=\boldsymbol{x}_{t}^{\prime}\boldsymbol{\beta}-\rho\left(\boldsymbol{x}_{t-1}^{\prime}\boldsymbol{\beta}\right)+\varepsilon_{t}$,
y definiendo como nuevas variables $y_{t}^{*}=y_{t}-\rho y_{t-1}$,
$x_{it}^{*}=x_{it}-\rho x_{it-1}$ el modelo se convierte en $y_{t}^{*}=\boldsymbol{x}_{t}^{*\prime}\boldsymbol{\beta}+\varepsilon_{t}$,
que no tiene heteroscedasticidad, y por tanto su estimador MCO coincide
con su estimador MCG. Aplicando el estimador as\'i calculado para realizar
una predicci\'on puntual, tenemos:

\[
\hat{y}_{t+1}^{*}=\boldsymbol{x}_{t+1}^{*\prime}\hat{\boldsymbol{\beta}}
\]


\[
\hat{y}_{t+1}=\boldsymbol{x}_{t+1}^{\prime}\hat{\boldsymbol{\beta}}+\rho\left(y_{t}-\boldsymbol{x}_{t}^{\prime}\hat{\boldsymbol{\beta}}\right)
\]


El segundo t\'ermino de la predicci\'on es un estimador de la esperanza
de $u_{t+1}$ condicionada a $u_{t}$ ya que $E\left(u_{t+1}|u_{t}\right)=\rho u_{t}=\rho\left(y_{t}-\boldsymbol{x}_{t}^{\prime}\boldsymbol{\beta}\right)$,
que se estima mediante $\rho\left(y_{t}-\boldsymbol{x}_{t}^{\prime}\hat{\boldsymbol{\beta}}\right)$.
La varianza de la predicci\'on ser\'a:

\[
Var\mbox{\ensuremath{\left(\hat{y}_{t}\right)=\hat{\sigma}_{\varepsilon}^{2}\left(1+\boldsymbol{x}_{t+1}^{*\prime}\left(\boldsymbol{X}^{*\prime}\boldsymbol{X}^{*}\right)^{-1}\boldsymbol{x}_{t+1}^{*}\right)}}
\]


Donde 
\[
\hat{\sigma}_{\varepsilon}^{2}=\dfrac{\left(y_{t}-\boldsymbol{X}^{*\prime}\hat{\boldsymbol{\beta}}\right)^{\prime}\left(y_{t}-\boldsymbol{X}^{*\prime}\hat{\boldsymbol{\beta}}\right)}{T-k}
\]


El problema es que normalmente $\rho$ es desconocido, y por tanto
debemos estimarlo conjuntamente con $\boldsymbol{\beta}$. Por tanto,
la predicci\'on ser\'ia:

\[
\hat{y}_{t+1}=\boldsymbol{x}_{t+1}^{\prime}\hat{\boldsymbol{\beta}}+\hat{\rho}\left(y_{t}-\boldsymbol{x}_{t}^{\prime}\hat{\boldsymbol{\beta}}\right)
\]


Pero ya no conocemos exactamente las propiedades de la predicci\'on
ni podemos expresar su varianza, ya que la varianza que hemos calculado
depende de $\rho$ y no tiene en cuenta la indertidumbre al estimarlo.
SI la varianza de la perturbaci\'on es mucho mayor que la varianza del
estimador, se puede utilizar $\hat{\sigma}_{\varepsilon}^{2}$ como
aproximaci\'on. Otra posibilidad es utilizar t\'ecnicas de bootstrapping
para establecer distribuciones muestrales.
