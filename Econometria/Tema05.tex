\chapter[Modelo lineal con series de tiempo.]{Modelo lineal con series de tiempo. \\
\normalsize  Variables binarias para efectos temporales y variables en forma de n\'umeros \'indice. Uso de variables con tendencia en la regresi\'on. Uso de series d\'ebilmente dependientes. Transformaci\'on de series altamente persistentes. Tratamiento de la estacionalidad en el modelo.}


\sectioncol{Modelo lineal con series de tiempo.}

La principal diferencia que presentan los modelos da series temporales respecto a los modelos de corte transvrersal es que los datos provenientes d eun conjnto de series temporales vienen ordenados respecto al tiempo, es decir, los datos tienen un orden natural. Esto es importante, porque los datos pasados pueden afectar a los datos futuros, pero no al rev\'es.

Otra distinci\'on fundamental es que, mientras que los datos de corte transversal se suponen provenientes de una muestra aleatoria tomada de una poblaci\'on mayor, en el ecaso de series temporales hemos de suponer que nuestros datos provienen de un proceso estoc\'astico o proceso de series temporales, es decir, una sucesi\'on de variables aleatorias cuyo \'indice es el tiempo. As\'i, un conjunto de datos de series temporales no es m\'as que una realizaci\'on del proceso estoc\'astico subyacente. Si fu\'esemos capaces de retorceder en el tiempo y volver a tomar los datos probablemente la realizaci\'on de ese proceso ser\'ia distinta, es por esto que consideramos a los datos como un conjunto de variables aleatorias. Por tanto, el conjunto de todas las posibles realizaciones de un proceso estoc\'astico equivale a la poblaci\'on total para un an\'alisis transversal, y el tama\~no muestral ser\'a el n\'umero de per\'iodos para los que disponemos de observaciones.

\subsectioncol{Tipos de modelos con series temporales.}

Vamos a ver dos tipos de modelos con series temporales que se pueden estimar f\'acilmente por m\'inimos cuadrados ordinarios.

\paragraph{Modelos est\'aticos.}

Es estos modelos, la relaci\'on entre las variables ex\'ogenas y la variable dependiente es contempor\'anea, esto es, tenemos datos de series temporales para un conjunto de variables, fechados de forma contempor\'anea y el modelo que las relaciona es del tipo $y_t=\beta_0+\sum_{i=1}^k\beta_ix_{ti}+u_t$.

Estos tipos de modelos se formulan cuando se entiende que los cambios en las variables ex\'ogenas tendr\'an un efecto inmediato en la variable dependiente, o cuando estamos interesados en conocer la relaci\'on de intercambio entre las variables.

\paragraph{Modelos de retardos distribuidos finitos.}

En un modelo de retardos distribu\'idos finitos, una o m\'as variables ex\'ogenas afectan a la variable dependientecon alg\'un retardo. Un ejemplo podr\'ia ser:

\[ y_t=\beta_0+\sum_{j=0}^{q}\delta_jx_{t-j}+u_t\]

Este modelo se utiliza para variables que influyen en la variable dependiente pero no de forma simult\'anea, sino con alg\'un retardo. El modelo que hemos puesto como ejempo es un modelo con retardos distribuidos finitos (RDF) de orden $q$.

Para interpretar este modelo, supongamos que $X_0$ es constante igual a una cantidad $c$, en  el instante  $t$ aumenta hasta $c+1$ y en el instante $t+1$ vuelve a bajar hasta $c$. En ese caso, el valor esperado de $y$ en cada instante ser\'ia:
\begin{align*}
 y_{t-1}=&\beta_0+\sum_{j=0}^{q}\delta_jc = y_{t-1} \\
 y_{t}=&\beta_0+\delta_0(c+1)+\sum_{j=1}^{q}\delta_jc = y_{t-1}+\delta_0 \\
 y_{t+1}=&\beta_0+\delta_0c+\delta_1(c+1)+\sum_{j=2}^{q}\delta_jc = y_{t-1}+\delta_1 \\
 y_{t+2}=&\beta_0+\delta_0c+\delta_1c+\delta2(c+1)+\sum_{j=3}^{q}\delta_jc = y_{t-1}+\delta_2 \\
 &\cdots \\
 y_{t+q}=&\beta_0+\sum_{j=0}^{q-1}\delta_jc+\delta_q(c+1) = y_{t-1} +\delta_q\\
  y_{t+q+1}=&\beta_0+\sum_{j=0}^{q}\delta_jc = y_{t-1}
\end{align*}

Por tanto, podemos ver que $\delta_0$ es el efecto inmediato que un cambio en $x_0$ tiene en $y$. Normalmente se denomina \textbf{propensi\'on al impacto} o \textbf{modificador de impacto}. Por otro lado, $\delta_1$ es el efecto en $y$ de un cambio en $x_0$ un per\'iodo despu\'es de que el cambio se produzca, $\delta_2$ es el efecto dos periodos despu\'es del cambio, etc . En el momento $t+q+1$ $y$ vuelve a su valor inicial, debido a que en nuestro modelo hemos supuesto $q$ retardos. Si realizamos un gr\'afico de $\delta_j$ respecto a $j$, obtenemos su distribuci\'on de retardos, que muestra el efecto que tiene sobre $y$ un cambio temporal en $x_0$.

Si el cambio en $x_0$ fuese permanente, es decir, si $x_0$ pasa de valer $c$ a valer $c+1$ para $t, t+1, \ldots$, el efecto ser\'ia el siguiente:

\begin{align*}
 y_{t-1}=&\beta_0+\sum_{j=0}^{q}\delta_jc = y_{t-1} \\
 y_{t}=&\beta_0+\delta_0(c+1)+\sum_{j=1}^{q}\delta_jc = y_{t-1}+\delta_0 \\
 y_{t+1}=&\beta_0+\delta_0(c+1)+\delta_1(c+1)+\sum_{j=2}^{q}\delta_jc = y_{t-1}+\delta_0+\delta_1 \\
 y_{t+2}=&\beta_0+\delta_0(c+1)+\delta_1(c+1)+\delta2(c+1)+\sum_{j=3}^{q}\delta_jc = y_{t-1}+\delta_0+\delta_1+\delta_2 \\
 &\cdots \\
 y_{t+q}=&\beta_0+\sum_{j=0}^{q}\delta_j(c+1) = y_{t-1}+\sum_{j=0}^{q}\delta_j \\
 y_{t+q+1}=&\beta_0+\sum_{j=0}^{q}\delta_j(c+1) = y_{t-1}+\sum_{j=0}^{q}\delta_j 
\end{align*}

Por tanto, vemos que $\sum_{j=0}^{q}\delta_j$ es el cambio a largo plazo que experimenta $y$ tras un aumento permanente de una unidad en $x_0$ y se denomina \textbf{propensi\'on a largo plazo (PLP)} o \textbf{multiplicador a largo plazo} y es a menudo de inter\'es en estos modelos.

Como a menudo existe una correlaci\'on elevada entre los retardos de la variable independiente, estos modelos pueden presentar problemas de multicolinealidad, lo que hace que las estimaciones de los $\delta_i$ individuales sean muy imprecisas. Veremos que a\'un en este caso, a menudo podemos obtener buenos estimadores de la PLP.

Estos modelos pueden tener m\'as de una variable con retardos, variables contempor\'aneas, etc. Puede ocurrir que el objetivo al estimar el modelo sea contrastar si la variable independiente tiene efecto retardado sobre la variable dependiente.

\subsectioncol{Propiedades del estimador MCO para muestras finitas.}

Veremos ahora c\'omo debemos modificar las hip\'otesis cl\'asicas del modelo para poderlas aplicar a un modelo de series temporales, y que propiedades se deducen de ellas para muestras finitas.

\paragraph{Insesgadez de los estimadores.}

La primera hip\'otesis cl\'asica establece que las variables se relacionan seg\'un un modelo lineal en los par\'ametros.

\begin{hipotesis}
\textbf{Linealidad en los par\'ametros:} El proceso estoc\'astico $\{(x_{t1},x_{t2},\ldots,x_{tk},y_t),t=1,2,\ldots,n\}$ sigue el modelo lineal
\[y_t=\beta_0+\beta_1x_{t1}+\beta_2x_{t2}+\cdots+\beta_kx_{tk}+u_t\]
donde $\{u_t,t=1,2,\ldots,n\}$ es la sucesi\'on de t\'erminos de error o perturbaciones.
\end{hipotesis}
 De igual modo que hemos hecho para el modelo con datos de corte transversal, podemos expresarlo en forma matricial, es decir, $\boldsymbol{y}=\boldsymbol{X\beta}+\boldsymbol{u}$.

\begin{hipotesis}
\textbf{Media condicionada nula:} Para cada instante $t$ el valor esperado del t\'ermino de error dadas las variables explicativas en todos los periodos temporales es cero, es decir,
\[E(u_t|\boldsymbol{X})=0\]
o, matricialmente:
\[E(\boldsymbol{u}|\boldsymbol{X})=\boldsymbol{0}\]
\end{hipotesis}

Esta hip\'otesis es crucial a la hora de deducir las propiedades que hacen al estimador MCO conveninete, y se puede interpretar en t\'erminos de incorrelaci\'on. Es decir, la hip\'otesis de esperanza ondicionada nula implica que el t\'ermino de error ert\'a incorrelacionado con las variables explicativas en cada uno de los periodos temporales. El hecho de que est\'e expresado en funci\'on de la esperanza condicionada implica que la relaci\'on entre la variable independiente y las variables explicativas debe estar completamente especificada en el modelo.

Si las $u_t$ son independientes de las $\boldsymbol{X}$ y $E(u_t)=0$ el supuesto se cumple autom\'aticamente.

Si lo que se cumple es $E(u_t|\boldsymbol{x}_t)=0$, decimos que tenemos exogeneidad contempor\'anea, pero la hip\'otesis exige exogeneidad estricta.

En el caso de datos de corte transversal, dado que suponemos muestreo aleatorio, la perturbaci\'on ser\'a autom\'aticamente independiete de las variables explicativas de otras observaciones. En el caso de series temporales esto no se produce, es por esto que debemos exigir expl\'icitamente la exogeneidad estricta.

La exogeneidad estricta puede incumplirse por varias causas: por una mala especificaci\'on del modelo que no incluya todas las variables independientes implicadas, por la comisi\'on de errores de medida al medir los regresores, o porque el valor de la variable independienta pueda influir en valores futuros de alguna variable ex\'ogena. Para que una variable explicativa sea estrictamente ex\'ogena, no puede verse influida por valores pasados de la variable end\'ogena. Por desgracia, esta circunstancia se da en muchos casos, y muchas variables explicativas violan la hip\'otesis de exogeneidad estricta.

Muchos ana\'alisis de modelos est\'aticos y de retardos distribuidos finitos suponen esta hip\'otesis mediante la hip\'otesisi m\'a fuerte de que las variables explicativas son deterministas, es decir, sin componente aleatorio, pero estop es obviamente falso si tenemos observaciones de series temporales.

\begin{hipotesis}
\textbf{No multicolinealidad perfecta:} En nuestro modelo ninguna variable independiente es cosntante o combinaci\'on lineal exacta de las dem\'as.
\end{hipotesis}

En el caso de que no se cumpla esta hip\'otesis, la matriz $\boldsymbol{X}$ ser\'a singular, y por tanto tendremos infinitos estimadores para los coeficientes del modelo.

\begin{teorema}
Bajo las anteriores hip\'otesis (linealidad, media condicionada nula y ausencia de multicolinealidad) los estimadores de m\'inimos cuadrados de los coeficientes del modelo son estimadores insesgados de los mismos.
\end{teorema}

Si se cumplen las hip\'otesis de linealidad y no multicolinealidad, el estimador MCO existe y su f\'ormula es $\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y}$. Para comprobar si es insesgado, calculemos su esperanza. Para ello, primero calculamos la esperanza condicionada a  $\boldsymbol{X}$:

Entonces: 
\begin{equation*}
\begin{array}{c}
\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y}=\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\left(\boldsymbol{X\beta}+\boldsymbol{u}\right)=\boldsymbol{\beta}+\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\boldsymbol{u}\\
E\left(\hat{\boldsymbol{\beta}}|\boldsymbol{X}\right)=E\left[\boldsymbol{\beta}+\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\boldsymbol{u}|\boldsymbol{X}\right]=\boldsymbol{\beta}+\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}E\left(\boldsymbol{u}|\boldsymbol{X}\right)=\boldsymbol{\beta}
\end{array}
\end{equation*}
Ya que $\boldsymbol{\beta}$ es un vector constante, aunque desconocido.

Como por la ley de las esperanzas totales, $E[E\left(\hat{\boldsymbol{\beta}}|\boldsymbol{X}\right)]=E\left(\hat{\boldsymbol{\beta}}\right)$, $E(\hat{\boldsymbol{\beta}})=\boldsymbol{\beta}$ y el estimador es insesgado, siempre que se cumpla la hip\'otesis de esperanza condicionada nula.

Para completar las hip\'otesis de Gauss-Markov para el caso de series temporales, necesitamos dos hip\'otesis adicionales.

\begin{hipotesis}
\textbf{Homoscedasticidad:} La varianza de la perturbaci\'on condicionada a $\boldsymbol{X}$ es constante para cualquier valor de $t$: $Var(u_t|\boldsymbol{X})=Var(u_t)=\sigma^2$, $t=1,2	\ldots,n$.
\end{hipotesis}

Por tanto, la varianza de la perturbaci\'on no puede depender de las variables ex\'ogenas (para lo que es suficiente que $u_t$ y $\boldsymbol{X}$ sean independientes) y esta varianza debe ser constante en el tiempo. Si esta hip\'otesis no se cumple, diremos que el modelo presenta \textit{heteroscedasticidad}, o bien que los errores son \textit{heterosced\'asticos}.

Para que esta hip\'otesis se cumpla, las variables no observadas que afecten a la variable dependiente deben tener una variabilidad constante, y que la varianza de la variable end\'ogena no dependa del valor de ninguna variable ex\'ogena.

\begin{hipotesis}
\textbf{Ausencia de autocorrelaci\'on:} Condicionando a las variables ex\'ogenas, los errores en dos periodos de tiempoe st\'an incorrelacionados: $Corr(u_t, u_s|\boldsymbol{X})=0$ para todo $t\neq s$.
\end{hipotesis}

Si esta hip\'otesis no se cumple, diremos que el t\'ermino de error presenta \textbf{autocorrelaci\'on}.

Si un modelo presenta homoscedasticidad y ausencia de autocorrelaci\'on, la matriz de varianzas-covarianzas del t\'ermino de error ser\'a:

\[Var(\boldsymbol{u}|\boldsymbol{X})=\sigma^2\boldsymbol{I}_n\]

Estas cinco hip\'otesis son las hip\'otesis de Gauss-Markov aplicadas al modelo con datos de series temporales.

\begin{teorema}
\textbf{Varianza del estimador MCO:} Bajo las hip\'otesis de Gauss-Markov (linealidad, media condicionada nula, ausencia de multicolinealidad, homocedasticidad y ausencia de autocorrelaci\'on) la matriz de varianzas-covarianzas de los estimadores condicionada a $\boldsymbol{X}$ es:

\[Var(\hat{\boldsymbol{\beta}})=\sigma^2(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\]

\end{teorema}


\begin{equation*}
\begin{array}{c}
Var\left(\hat{\boldsymbol{\beta}}|\boldsymbol{X}\right)=E\left[\left(\hat{\boldsymbol{\beta}}-E\left(\hat{\boldsymbol{\beta}}\right)\right)\left(\hat{\boldsymbol{\beta}}-E\left(\hat{\boldsymbol{\beta}}\right)\right)^{\prime}|\boldsymbol{X}\right]=E\left[\left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right)\left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right)^{\prime}|\boldsymbol{X}\right]=\\
=E\left[\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\boldsymbol{u}\boldsymbol{u}^{\prime}\boldsymbol{X}\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}|\boldsymbol{X}\right]=\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}E\left(\boldsymbol{u}\boldsymbol{u}^{\prime}|\boldsymbol{X}\right)\boldsymbol{X}\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}=\\
=\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{\prime}\sigma_{u}^{2}\boldsymbol{I}_{N}\boldsymbol{X}\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}=\sigma_{u}^{2}\left(\boldsymbol{X}^{\prime}\boldsymbol{X}\right)^{-1}
\end{array}
\end{equation*}

La diagonal principal de esta matriz nos da la varianza de cada uno de los estimadores de los par\'ametros del modelo. Veamos qu\'e factores influyen en estas varianzas:

\begin{itemize}
\item Cuanto mayor sea la varianza de la perturbaci\'on, mayor ser\'a la varianza de los estimadores. Esto es lo esperado, cuanto m\'as se aparte nuestro sistema del modelo que hemos especificado, menos precisos ser\'an los estimadores.
\item Cuanto mayor sea la dispersi\'on de las variables explicativas, o mayor sea el tama\~no de la muestra, menor ser\'a la varianza, ya que la matriz est\'a dividiendo. Esto es l\'ogico pues por un lado, cuanto m\'as repartida est\'e la muestra por el rango de variaci\'on posible m\'as informaci\'on capturaremos, y a mayor tama\~no de la muestra m\'as eficiente ser\'a nuestro estimador.
\item Cuanta menos multicolineanlidad presenten las variales explicativas, menor ser\'a la varianza. Si las variables explicativas presentan un comportamiento muy cercano a la multicolinealidad, la matriz ser\'a muy pr\'oxima a ser singular, con un determinante pr\'oximo a cero. Por tanto, su inversa ser\'a muy grande y por tanto las varianzas tambi\'en, dando origen a estimadores muy poco eficientes.
\end{itemize}

\begin{teorema}
\textbf{Estimador insesgado de $\sigma^2$:} Bajo las hip\'otesis de Gauss-Markov (linealidad, media condicionada nula, ausencia de multicolinealidad, homocedasticidad y ausencia de autocorrelaci\'on) el estimador

\[Var(\hat{\sigma}^2)=\dfrac{\boldsymbol{u}^{\prime}\boldsymbol{u}}{n-k}\]

es un estimador insesgado de $\sigma^2$.

\end{teorema}

\begin{teorema}
\textbf{Teorema de Gauss-Markov:} Bajo las hip\'otesis de Gauss-Markov (linealidad, media condicionada nula, ausencia de multicolinealidad, homocedasticidad y ausencia de autocorrelaci\'on) los estimadores de m\'inimos cuadrados ordinarios son los estimadores lineales insesgados de m\'inima varianza, condicionados a $\boldsymbol{X}$.

\end{teorema}

As\'i, los estimadores tiene las mismas propiedades para muestras finitas que con la ship\'otesis para datos de secci\'on cruzada.

\subsectioncol{Inferencia bajo los supuestos del modelo lineal cl\'asico.}

Si a las hip\'otesis anteriores les agregamos la siguiente hip\'otesis:

\begin{hipotesis}
\textbf{Normalidad de la perturbaci\'on:} Los errores $u_t$ son independientes de $\boldsymbol{X}$ y est\'an independiente e id\'enticamente distribuidos seg\'un una distribuci\'on $N(0;\sigma)$.
\end{hipotesis}

Esta hip\'otesis implica las hip\'otesis de media condicionada nula, heteroscedasticidad y ausencia de correlaci\'on, pero es m\'as restrictiva. Incluy\'endola tenemos las hip\'otesis correspondientes al modelo lineal cl\'asico, y podemos ennunciar el siguiente teorema:

\begin{teorema}
\textbf{Distribuciones muestrales normales:} Bajo las hip\'otesis del modelo lineal cl\'asico (linealidad, media condicionada nula, ausencia de multicolinealidad, homocedasticidad, ausencia de autocorrelaci\'on y normalidad) los estimadores de m\'inimos cuadrados ordinarios se distribuyen siguiendo una distribuci\'on normal, condicionados a $\boldsymbol{X}$. Adem\'as, bajo la hip\'otesis nula cada estad\'istico $t$ sigue una $t$ de Student, y cada estad\'istico $F$ sigue una $F$ de Snedecor. La construcci\'on habitual de intervalos de confianza sigue siendo v\'alida.
\end{teorema}

Este teorema implica que si se cumplen las hip\'otesis, todos los resultados obtenidos para datos de corte transversal se pueden aplicar a modelos de series temporales. Estos supuestos son mucho m\'as restrictivos para datos de series temporales, en particular la exogeneidad estricta y la ausencia de autocorrelaci\'on raramente se cumplen.

\sectioncol{Variables binarias para efectos temporales y variables en forma de n\'umeros \'indice. }


Seccion 10.4, 
\sectioncol{Uso de variables con tendencia en la regresi\'on.}
Seccion 10.5
\sectioncol{Uso de series d\'ebilmente dependientes.}
Seccion 11.1
\sectioncol{Transformaci\'on de series altamente persistentes.}
Seccion 11.3
\sectioncol{Tratamiento de la estacionalidad en el modelo.}
Seccion 10.5