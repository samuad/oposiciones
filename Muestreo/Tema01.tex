\selectlanguage{english}%

\chapter{Concepto de poblaci\'on, marco y muestra. Muestreo probabil\'istico.
Distribuci\'on de un estimador en el muestreo. Error cuadr\'atico medio
y sus componentes. Intervalos de confianza: Estimadores insesgados
y sesgados. M\'etodos de selecci\'on. Probabilidad de la unidad de pertenecer
a la muestra y propiedades. Comparaci\'on con el muestreo no probabil\'istico:
Muestreo por cuotas.}


\section{Concepto de poblaci\'on, marco y muestra.}

Cuando queremos estudiar una serie de caracter\'isticas en una poblaci\'on
podemos abordar el estudio de dos formas:
\begin{itemize}
\item Observando dichas caracter\'isticas en todos los componentes de esa
poblaci\'on. En ese caso se dice que estamos realizando un censo. En
muchas ocasiones realizar censos no es posible, bien por motivos de
coste, porque para observar un elemento de la poblaci\'on haya que destruirlo,
porque la poblaci\'on sea infinita o por otras causas.
\item Observando un conjunto de elementos de la poblaci\'on y estrapolando
la informaci\'on qe obtengamos al resto de la poblaci\'on. En este caso
se dice que estamos realizando una encuesta. Al conjunto de t\'ecnicas
matem\'aticas para seleccionar ese subconjunto de la poblaci\'on de forma
que sea representativo de la misma se le llama muestreo. A las t\'ecnicas
para deducir informaci\'on acerca de la poblaci\'on a trav\'es de un subconjunto
de la misma se le llama inferencia estad\'istica.\end{itemize}
\begin{description}
\item [{Poblaci\'on~objetivo:}] Colecci\'on de elementos sobre los que se
desea investigar una determinada caracter\'istica.
\item [{Poblaci\'on~investigada:}] Subconjunto de la poblaci\'on objetivo
que realmente es objeto de estudio. La parte no estudiada puede ser
por no poder acceder a ella, negativas, etc.
\item [{Unidad~elemental~de~muestreo:}] Cada elemento de la poblaci\'on
investigada.
\item [{Unidad~de~muestreo~compuesta:}] Conjunto de varias unidades
elementales que se investigan conjuntamente.
\item [{Marco:}] Listado de todas las unidades de muestreo, que se utilizar\'a
para seleccionar la muestra. Lo ideal es que el marco coincida con
la poblaci\'on objetivo, pero puede no ser as\'i debido a errores, falta
de actualizaciones, omisiones y otras causas. La diferencia entre
el marco y la poblaci\'on objetivo debe ser lo suficientemente peque\~na
como para que las inferencias que se hagan sobre la poblaci\'on sean
v\'alidas.
\item [{Muestra:}] Colecci\'on o conjunto de unidades de muestreo seleccionada
del marco correspondiente a la poblaci\'on que se quiere investigar.
\item [{Tama\~no~muestral:}] N\'umero de elementos que componen la muestra.
\end{description}
Se llama \textbf{marco en sentido amplio} a aquel que adem\'as de incluir
el listado de las unidades de la poblaci\'on a investigar incluye informaci\'on
adicional sobre las mismas. Esta informaci\'on puede utilizarse para
mejorar el dise\~no de los procesos de muestreo. Como ejemplos de informaci\'on
adicional podemos citar variables auxiliares que tengan alguna correlaci\'on
con la variable en estudio, o el conocimiento de estimaciones de caracter\'isticas
de la poblaci\'on provenientes de una encuesta anterior.

Si nuestro marco consta de unidades de muestreo compuestas, y tenemos
listados parciales de las unidades simples que componen cada unidad
compuesta, se dice que dispondremos de marcos m\'ultiples.

Si el muestreo se hace de forma que se conoce la probabilidad de obtener
cada una de las muestras posibles, se dice que el muestreo es probabil\'istico.
Si la poblaci\'on objetivo es finita, se habla de muestreo en poblaciones
finitas.


\section{Muestreo probabil\'istico.}

Un m\'etodo de muestreo es un mecanismo mediante el que seleccionaremos
la muestra a partir de la poblaci\'on a investigar.

El m\'etodo de muestreo que hayamos definido se llama muestreo probabil\'istico
si cumple las siguientes condiciones:
\begin{itemize}
\item Podemos definir el conjunto de muestras distintas posibles que generar\'a
el m\'etodo de muestreo aplicado a una poblaci\'on espec\'ifica. Esto quiere
decir que podemos especificar las unidades de la poblaci\'on que pertenecen
a cada muestra de las posibles.
\item Cada muestra posible tiene asignada una probabilidad de selecci\'on. 
\end{itemize}
Es f\'acil comprobar que en este supuesto, el procedimiento de muestreo
aplicado a una poblaci\'on es un fen\'omeno aleatorio probabilizable.
En general s\'olo se considerar\'an m\'etodos de muestreo en los que no
haya ninguna muestra con probabilidad nula, es decir, m\'etodos de muestreo
no restringidos.

En la pr\'actica, rara vez se define un procedimiento mediante todas
las muestras posibles y sus probabilidades de obtenci\'on, ya que esto
ser\'ia muy laborioso a poco que la poblaci\'on investigada fuese grande.
En su lugar, y de forma equivalente, se define el procedimiento de
muestreo asignando a cada unidad de muestreo la probabilidad de que
est\'e incluida en la muestra.

Formalmente, el procedimiento de muestreo ser\'a probabil\'istico si,
siendo $S$ el conjunto de todas las posibles muestras a obtener mediante
nuestro procedimiento de muestreo $\left\{ s_{1},s_{2},\ldots,s_{n}\right\} $y
siendo $\mathcal{F}$ la familia de todos los subconjuntos posibles
de $S$, forman un espacio probabilizable, y adem\'as la funci\'on que
asigna a cada muestra posible su probabilidad de obtenci\'on es una
medida de probabilidad sobre este espacio.


\section{Distribuci\'on de un estimador en el muestreo.}

Una vez hemos definido un procedimiento de muestro probabil\'istico,
debemos construir un estimador que nos permita inferir a partir de
la muestra seleccionada las caracter\'isticas poblacionales a investigar
(total, media, proporci\'on, etc). Estos estimadores ser\'an por tanto
variables aleatorias cuya distribuci\'on de probabilidad ser\'a funci\'on
de las probabilidades de las muestras.

M\'as formalmente, sea la caracter\'istica $X$ de los elementos de la
poblaci\'on $U$, que toma el valor $X_{i}$ para cada unidad $U_{i}$
de la poblaci\'on. Consideramos ahora una funci\'on $\theta$ de los valores
$X_{i}$, a la que llamaremos par\'ametro poblacional. Seleccionamos
una muestra aleatoria $s$ de nuestra poblaci\'on, y a partir de los
valores de la caracter\'istica en los elementos de la muestra queremos
obtener una estimaci\'on de $\theta$ para el total de la poblaci\'on,
mediante una funci\'on $\hat{\theta}$ basada en los valores que toma
la caracter\'istica en los elementos de la muestra.

A la funci\'on $\hat{\theta}$ que asocia cada muestra a un valor num\'erico
se le llama estimador del par\'ametro poblacional $\theta$, y alos
valores de dicha funci\'on para cada muestra se les llama estimaciones.
As\'i, podemos formalizar el estimador como una aplicaci\'on medible del
espacio de todas las muestras posibles a $\mathbb{R}$,$\hat{\theta}:S\rightarrow\mathbb{R}$
y por tanto se puede definir $\hat{\theta}$ como una variable aleatoria
sobre la recta real.A la funci\'on de probabilidad inducida por la variable
aleatoria $\hat{\theta}$ se la llama distribuci\'on de probabilidad
en el muestreo del estimador $\hat{\theta}$. Por tanto, sea $T=\left\{ t\in\mathbb{R}/\exists\left(X_{1},\ldots,X_{n}\right)\in S\left(X\right),\hat{\theta}\left(X_{1},\ldots,X_{n}\right)=t\right\} $.
El subconjunto $T$ de $\mathbb{R}$ constituye el conjunto de valores
del estimador. Se define la ley de probabilidad del estimador como
$P\left(\hat{\theta}\left(X_{1},\ldots,X_{n}\right)=t\right)=\sum_{\left\{ S_{i}/\hat{\theta}\left(S_{i}\left(X\right)\right)=t\right\} }P\left(S_{i}\right)$,
es decir, la probabilidad de cada valor del estimador es igual a la
suma de las probabilidades de obtener cada muestra que origine ese
mismo valor del estimador. Por tanto, la distribuci\'on de probabilidad
del estimador en el muestreo ser\'a el conjunto de pares formados por
todos los valores posibles del estimador y las probabilidades de que
el estimador tome esos valores.

Frecuentemente se expresa un procedimiento de estimaci\'on como el conjunto
formado por el marco, el procedimiento de muestreo y el estimador
utilizado. Al conjunto formado por las muestras posibles, sus probabilidades
y el estimador se le llama dise\~no muestral.

A partir de la definici\'on de muestreo probabil\'istico y de distribuci\'on
de un estimador en el muestreo, podemos utilizar las herramientas
de la inferencia estad\'istica para obtener propiedades de los estimadores.


\section{Error cuadr\'atico medio y sus componentes.}

Dado que nuestro estimador es una variable aleatoria, podemos definir
para el mismo todas las propiedades asignadas a las variables aleatorias.

Definimos la esperanza matem\'atica o media del estimador $\hat{\theta}$
del par\'ametro poblacional $\theta$ como $E\left(\hat{\theta}\right)=\sum_{s}\hat{\theta}\left(s_{i}\right)P\left(s_{i}\right)=\sum_{\mathbb{R}}tP\left(\hat{\theta}=t\right)$.

Definimos la varianza del estimador $\hat{\theta}$ del par\'ametro
poblacional $\theta$ como $V\left(\hat{\theta}\right)=\sigma_{\hat{\theta}}^{2}=E\left[\left(\hat{\theta}-E\left(\hat{\theta}\right)\right)^{2}\right]=\sum_{\mathbb{R}}\left(t-E\left(\hat{\theta}\right)\right)^{2}P\left(\hat{\theta}=t\right)=E\left(\hat{\theta}^{2}\right)-\left(E\left(\hat{\theta}\right)\right)^{2}$.
Es una medida de la concentraci\'on de los valores del estimador en
torno a su valor medio.

Definimos el error de muestreo o desviaci\'on t\'ipica del estimador como
la ra\'iz cuadrada de su varianza.

Definimos el error relativo de muestreo, o coeficiente de variaci\'ondel
estimador, como su desviaci\'on t\'ipica dividido por su esperanza.

Definimos el sesgo del estimador $\hat{\theta}$ del par\'ametro poblacional
$\theta$ como la diferencia entre el valor esperado del estimador
y el valor real del par\'ametro: $B\left(\hat{\theta}\right)=E\left(\hat{\theta}\right)-\theta$.
Se dice que el estimador es insesgado si su sesgo es cero, y sesgado
si su sesgo es distinto de cero. El estimador es consistente cuando
su sesgo tiende a cero al aumentar el tama\~no de la muestra.

Definimos acuracidad o error cuadr\'atico medio del estimador $\hat{\theta}$
del par\'ametro poblacional $\theta$ como $ECM\left(\hat{\theta}\right)=E\left[\left(\hat{\theta}-\theta\right)^{2}\right]$.

La precisi\'on de un estimador se analiza en funci\'on de su error de
muestreo, su error cuadr\'atico medio y su sesgo. El error cuadr\'atico
medio se puede descomponer de la siguiente forma: 
\[
\begin{array}{c}
ECM\left(\hat{\theta}\right)=E\left[\left(\hat{\theta}-\theta\right)^{2}\right]=E\left[\left(\hat{\theta}-E\left(\hat{\theta}\right)+E\left(\hat{\theta}\right)-\theta\right)^{2}\right]=E\left[\left(\hat{\theta}-E\left(\hat{\theta}\right)+B\left(\hat{\theta}\right)\right)^{2}\right]=\\
=E\left[\left(\hat{\theta}-E\left(\hat{\theta}\right)\right)^{2}\right]+\left(B\left(\hat{\theta}\right)\right)^{2}+2B\left(\hat{\theta}\right)E\left(\hat{\theta}-E\left(\hat{\theta}\right)\right)=Var\left(\hat{\theta}\right)+\left(B\left(\hat{\theta}\right)\right)^{2}=\sigma_{\hat{\theta}}^{2}+\left(B\left(\hat{\theta}\right)\right)^{2}
\end{array}
\]


As\'i, el error cuadr\'atico medio se puede descomponer como la suma de
la varianza del estimador m\'as el cuadrado de su sesgo, o, expresado
de otra forma, la suma de los cuadrados del error de muestreo y del
sesgo. En general, es conveniente que el error cuadr\'atico medio sea
lo m\'as peque\~no posible. Es deseable obtener estimadores insesgados,
siempre que eso no implique un aumento de su varianza. As\'imismo, es
deseable obtener estimadores de varianza peque\~na, siempre que eso
no implique la aparici\'on de un sesgo. En la pr\'actica, se puede admitir
el uso de estimadores sesgados siempre que el cociente $\left|\dfrac{B\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}\right|$
sea igual o menor que $0,1$. Para poder comparar distintos estimadores,
y elegir el m\'as conveniente para estimar nuestro par\'ametro, debemos
distinguir entre estimadores sesgados e insesgados.


\subsection{Comparaci\'on de estimadores insesgados.}

Si un estimador es insesgado, su error cuadr\'atico medio coincide con
su varianza. Por tanto, para comparar varios estimadores insesgados
del mismo par\'ametro, hay que considerar sus errores de muestreo, siendo
mejor el estimador cuyo error de muestreo sea menor. Adem\'as, el error
relativo s\'olo var\'ia en funci\'on del error de muestreo, por tanto podemos
hacer depender la decisi\'on s\'olo del error de muestreo.


\subsection{Comparaci\'on de estimadores sesgados.}

Si un estimador es sesgado, la magnitud para analizar su precisi\'on
es su error cuadr\'atico medio. Por lo tanto, para comparar varios estimadores
sesgados en cuanto a precisi\'on se utiliza el ECM, y ser\'a mejor el
que menor ECM presente.

Pero en la pr\'actica el c\'alculo del error cuadr\'atico medio puede ser
problem\'atico, e incluso su valor puede variar dependiendo del valor
del par\'ametro que se investiga. Es por esto que se utiliza el siguiente
razonamiento:

Sabemos que $ECM\left(\hat{\theta}\right)=\sigma_{\hat{\theta}}^{2}+\left(B\left(\hat{\theta}\right)\right)^{2}$,
y por tanto, podemos formar un tri\'angulo rect\'angulo en el que $\sigma_{\hat{\theta}}$
y $B\left(\hat{\theta}\right)$ sea la longitud de los catetos, y
$\sqrt{ECM\left(\hat{\theta}\right)}$ la de la hipotenusa. Por tanto,
la tangente del \'angulo que forma la hipotenusa con el cateto de longitud
$\sigma_{\hat{\theta}}$, $\alpha$, ser\'a: $\tan\alpha=\dfrac{B\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}$,
que es la contribuci\'on del sesgo y la desviaci\'on t\'ipica al error cuadr\'atico
medio. Por tanto, cuanto menor sea este cociente, menor ser\'a la contribuci\'on
del sesgo al ECM.

Por esta raz\'on, se calcula el cociente $\left|\dfrac{B\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}\right|$
para cada estimador a comparar, siendo m\'as preciso el estimador que
presenta una relaci\'on del sesgo al error de muestreo m\'as peque\~na.
Tambi\'en se puede utitlizar el coeficiente de variaci\'on, siendo el
mejor estimador el que menor coeficiente de variaci\'on presente.

Si los estimadores a comparar tienen un sesgo despreciable, es decir,$\left|\dfrac{B\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}\right|<\dfrac{1}{10}$
se consideran insesgados.


\section{Estimaci\'on por intervalos de confianza.}

Si realizamos una afirmaci\'on acerca de un par\'ametro poblacional a
partir de la informaci\'on contenida en una muestra, lo podemos hacer
bas\'andonos e el valor puntual de un estad\'istico basado en la misma,
o bien mediante un intervalo de confianza. Un intervalo de confianza
al nivel de confianza $\alpha$ es un intervalo real para el que hay
una probabilidad $1-\alpha$ de que contenga al valor real del par\'ametro.
Al valor $1-\alpha$ se le suele llamar coeficiente de confianza.
Veremos c\'omo utilizar estimadores para hallar intervalos de confianza.


\subsection{Intervalos de confianza si el estimador es insesgado.}

Se trata de construir un intervalo de confianza para el par\'ametro
$\theta$ mediante un estimador insesgado del mismo, es decir, $E\left(\hat{\theta}\right)=\theta$.


\subsubsection{El estimador tiene una distribuci\'on normal.}

En ese caso, si $\hat{\theta}\sim N\left(\theta,\sigma_{\hat{\theta}}^{2}\right)$,
entonces $E\left(\hat{\theta}\right)=\theta$, y $\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\sim N\left(0,1\right)$
y se puede calcular $\lambda_{\alpha}$ tal que $P\left[-\lambda_{\alpha}\leq\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]=1-\alpha$
\[
\begin{array}{c}
P\left[\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]-P\left[\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\leq-\lambda_{\alpha}\right]=1-\alpha\\
P\left[\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]-\left\{ 1-P\left[\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\geq-\lambda_{\alpha}\right]\right\} =1-\alpha\\
P\left[\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]-\left\{ 1-P\left[\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]\right\} =1-\alpha\\
2P\left[\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]-1=1-\alpha\\
P\left[\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]=1-\dfrac{\alpha}{2}
\end{array}
\]
 por tanto, $\lambda_{\alpha}$ es el valor que hace que la funci\'on
de distribuci\'on de la normal est\'andar valga $1-\dfrac{\alpha}{2}$,
y el intervalo de confianza ser\'a:
\[
\hat{\theta}-\lambda_{\alpha}\sigma_{\hat{\theta}}\leq\theta\leq\hat{\theta}+\lambda_{\alpha}\sigma_{\hat{\theta}}
\]


Lo normal es no conocer el valor de $\sigma_{\hat{\theta}}$, sino
de una estimaci\'on del mismo con datos muestrales conocidos. En estos
casos no podemos asegurar con exactitud que el intervalo de confianza
es como hemos visto. Para estos casos, podemos usar la distribuci\'on
t de Student con $n-1$ grados de libertad para calcular el intervalo
de confianza. En este caso, tenemos que $\dfrac{\hat{\theta}-\theta}{\hat{\sigma}_{\hat{\theta}}}\sim t_{n-1}$,
y de forma similar, el intervalo de confianza ser\'a $\hat{\theta}-\lambda_{\alpha}\hat{\sigma}_{\hat{\theta}}\leq\theta\leq\hat{\theta}+\lambda_{\alpha}\hat{\sigma}_{\hat{\theta}}$
solo que en este caso $\lambda_{\alpha}$ es el valor que hace que
la funci\'on de distribuci\'on t de Student con $n-1$ grados de libertad
valga $1-\dfrac{\alpha}{2}$.


\subsubsection{El estimador no tiene una distribuci\'on normal.}

En este caso utilizamos la desigualdad de Tchebichev, que dice: 
\[
P\left\{ \left|\hat{\theta}-E\left(\hat{\theta}\right)\right|<k\right\} \geq1-\dfrac{\sigma_{\hat{\theta}}^{2}}{k^{2}}\,\,\,\,\forall k>0
\]


Como $\hat{\theta}$ es insesgado para $\theta$, $E\left(\hat{\theta}\right)=\theta$
y $P\left\{ \left|\hat{\theta}-\theta\right|<k\right\} \geq1-\dfrac{\sigma_{\hat{\theta}}^{2}}{k^{2}}$.
Para un nivel de significaci\'on $\alpha$ tomamos $k=\dfrac{\sigma_{\hat{\theta}}}{\sqrt{\alpha}}$,
y entonces $P\left\{ \left|\hat{\theta}-\theta\right|<\dfrac{\sigma_{\hat{\theta}}}{\sqrt{\alpha}}\right\} \geq1-\alpha$,
y el intervalo ser\'a $\hat{\theta}-\dfrac{\sigma_{\hat{\theta}}}{\sqrt{\alpha}}\leq\theta\leq\hat{\theta}+\dfrac{\sigma_{\hat{\theta}}}{\sqrt{\alpha}}$.
En general, este intervalo es m\'as ancho que si la distribuci\'on es
normal, por lo que la propiedad de normalidad es bastante deseable.


\subsection{Estimadores sesgados.}

Si el estimador $\hat{\theta}$ es sesgado para $\theta$, $B\left(\hat{\theta}\right)=E\left(\hat{\theta}\right)-\theta\neq0$
. Por el teorema central del l\'imite, si la muestra es suficientemente
grande, se cumple $\dfrac{\hat{\theta}-E\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}\sim N\left(0,1\right)$
, y por tanto, $ $
\[
\begin{array}{c}
P\left[-\lambda_{\alpha}\leq\dfrac{\hat{\theta}-E\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]=1-\alpha\Rightarrow P\left[-\lambda_{\alpha}\leq\dfrac{\hat{\theta}-\theta-B\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]=1-\alpha\Rightarrow\\
\Rightarrow P\left[-\lambda_{\alpha}\leq\dfrac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}-\dfrac{B\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}\leq\lambda_{\alpha}\right]=1-\alpha
\end{array}
\]
 y en el c\'alculo del intervalo aparece en t\'ermino$\dfrac{B\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}$.
Como ya sabemos, si $\left|\dfrac{B\left(\hat{\theta}\right)}{\sigma_{\hat{\theta}}}\right|<\dfrac{1}{10}$
la influencia del sesgo es despreciable con lo que el intervalo de
confianza es el mismo que para estimadores insesgados. Si esto no
se cumple, el sesgo influir\'a en el intervalo de confianza, y se tiene
que el intervalo es $\hat{\theta}-\lambda_{\alpha}\hat{\sigma}_{\hat{\theta}}-B\left(\hat{\theta}\right)\leq\theta\leq\hat{\theta}+\lambda_{\alpha}\hat{\sigma}_{\hat{\theta}}-B\left(\hat{\theta}\right)$.
Ahora el intervalo no est\'a centrado en $\hat{\theta}$, est\'a desplazado
en una cantidad $B\left(\hat{\theta}\right)$ respecto al intervalo
sin sesgo. Si queremos centrar el intervalo en torno a $\hat{\theta}$
tenemos que tomar el peor de los casos, y alargar el extremo del intervalo
m\'as p\'orximo a$\hat{\theta}$ hast conseguir un intervalo sim\'etrico.
Por tanto, el intervalo ser\'a m\'as largo que para un estimador sin sesgo.


\section{M\'etodos de selecci\'on. Probabilidad de la unidad de pertenecer a la
muestra y propiedades.}

Veremos ahora las distintas formas en las que se pueden seleccionar
las unidades que pertenecen a la muestra. Inicialmente se pueden clasificar
en dos grandes clases: m\'etodos de muestreo con reposici\'on y m\'etodos
de muestreo sin reposici\'on.

Un procedimiento aleatorio de muestreo es sin reposici\'on si todas
las muestras que tienen alg\'un elemento repetido son imposibles. Es
decir, las unidades seleccionadas no se devuelven a la poblaci\'on para
seleccionar la siguiente unidad de la muestra, de ah\'i el nombre. Como
norma general, no se tiene en cuenta el modo de colocar los elementos
en la muestra, es decir, muestras con los mismos elementos en distinto
orden son consideradas iguales. Por tanto, el espacio muestral contiene
$C_{N,n}=\dbinom{N}{n}$ muestras de tama\~no $n$ distintas posibles
en una poblaci\'on de tama\~no $N$. Si las muestras con los mismos elementos
colocados en distinto orden son distintasm el espacio muestral contiene
$V_{N.n}=\dbinom{N}{n}n!$ muestras.

Un procedimiento aleatorio de muestreo es con reposici\'on cuando las
muestras que tienen alg\'un elemento repetido son posibles. Es decir,
las unidades seleccionadas se devuelven a la poblaci\'on para seleccionar
la siguiente unidad de la muestra. Si se tiene en cuenta el orden
de colocaci\'on en la muestra, el espacio muestral tendr\'a $VR_{N,n}=N^{n}$
muestras posibles. Si no se tiene en cuenta el orden, el espacio muestral
tiene $CR_{N,n}=\dbinom{N-n-1}{n}$ muestras posibles.

Adicionalmente, se pueden clasificar los m\'etodos de selecci\'on en otros
dos grandes grupos:
\begin{description}
\item [{Selecci\'on~con~probabilidades~iguales:}] Todos los elementos
de la poblaci\'on tienen la misma probabilidad de pertenecer a la muestra.
\item [{Selecci\'on~con~probabilidades~desiguales:}] Los elementos de
la poblaci\'on no tienen la misma probabilidad de pertenecer a la muestra.
\end{description}

\subsection{Selecci\'on sin reposici\'on.}

Consideremos una poblaci\'on de tama\~no $N$, con unidades $\left\{ u_{1},u_{2},\ldots,u_{N}\right\} $.
Seleccionamos sin reposici\'on una muestra $\left(\tilde{\boldsymbol{x}}\right)$
de tama\~no $n$. Por tanto, cada unidad puede aparecer como m\'aximo
una vez en la muestra. Para cada unidad de la poblaci\'on, $u_{i}$,
definimos la variable aleatoria $e_{i}$ como sigue: 
\[
e_{i}=\begin{cases}
1 & si\,u_{i}\in\left(\tilde{\boldsymbol{x}}\right)\\
0 & si\,u_{i}\notin\left(\tilde{\boldsymbol{x}}\right)
\end{cases}
\]
 y con la distribuci\'on de probabilidad $P\left(e_{i}=1\right)=\pi_{i}$,
$P\left(e_{i}=0\right)=1-\pi_{i}$. As\'i hemos definido una variable
aleatoria en funci\'on de la probabilidad de que la unidad $i$-\'esima
correspondiente pertenezca a la muestra. Las propiedades de esta variabla
aleatoria (que presenta una distribuci\'on de Bernouilli) son: 
\[
\begin{array}{cc}
E\left(e_{i}\right)=1\pi_{i}+0\left(1-\pi_{i}\right)=\pi_{i} & E\left(e_{i}^{2}\right)=1^{2}\pi_{i}+0^{2}\left(1-\pi_{i}\right)=\pi_{i}\\
Var\left(e_{i}\right)=E\left(e_{i}^{2}\right)-\left[E\left(e_{i}\right)\right]^{2}=\pi_{i}-\pi_{i}^{2}=\pi_{i}\left(1-\pi_{i}\right)
\end{array}
\]


Ahora consideramos la variable aleatoria producto, $e_{i}e_{j}$ con
$i\neq j$, que evidentemente se define: 
\[
e_{i}e_{j}=\begin{cases}
1 & si\,\left(u_{i},u_{j}\right)\in\left(\tilde{\boldsymbol{x}}\right)\\
0 & si\,\left(u_{i},u_{j}\right)\notin\left(\tilde{\boldsymbol{x}}\right)
\end{cases}
\]
 y con la distribuci\'on de probabilidad $P\left(e_{i}e_{j}=1\right)=\pi_{ij}$,
$P\left(e_{i}e_{j}=0\right)=1-\pi_{ij}$. Entonces, 
\[
\begin{array}{c}
E\left(e_{i}e_{j}\right)=1\pi_{ij}+0\left(1-\pi_{ij}\right)=\pi_{ij}\\
Cov\left(e_{i},e_{j}\right)=E\left(e_{i}e_{j}\right)-E\left(e_{i}\right)E\left(e_{j}\right)=\pi_{ij}-\pi_{i}\pi_{j}
\end{array}
\]


Veamos algunas propiedades de estas probabilidades:
\begin{enumerate}
\item $\sum_{i=1}^{N}\pi_{i}=n$ ya que $\sum_{i=1}^{N}\pi_{i}=\sum_{i=1}^{N}E\left(e_{i}\right)=E\left(\sum_{i=1}^{N}e_{i}\right)=E\left(n\right)=n$.
\item $\sum_{i=1,i\neq j}^{N}\pi_{ij}=\left(n-1\right)\pi_{j}$, ya que
$\sum_{i=1,i\neq j}^{N}\pi_{ij}=\sum_{i=1,i\neq j}^{N}E\left(e_{i}e_{j}\right)=E\left(\sum_{i=1,i\neq j}^{N}\left(e_{i}e_{j}\right)\right)=E\left(e_{j}\left(\sum_{i=1}^{N}e_{i}-e_{j}\right)\right)=nE\left(e_{j}\right)-E\left(e_{j}^{2}\right)=\left(n-1\right)\pi_{j}$.
\item $\sum_{i=1,i\neq j}^{N}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)=-\pi_{j}\left(1-\pi_{j}\right)$,
ya que $\sum_{i=1,i\neq j}^{N}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)=\sum_{i=1,i\neq j}^{N}\pi_{ij}-\sum_{i=1,i\neq j}^{N}\pi_{i}\pi_{j}=\left(n-1\right)\pi_{j}-\pi_{j}\left(\sum_{i=1}^{N}\pi_{i}-\pi_{j}\right)=$
$\left(n-1\right)\pi_{j}-n\pi_{j}-\pi_{j}^{2}=-\pi_{j}\left(1-\pi_{j}\right)$.
\end{enumerate}

\subsection{Selecci\'on con reposici\'on.}

Consideremos una poblaci\'on de tama\~no $N$, con unidades $\left\{ u_{1},u_{2},\ldots,u_{N}\right\} $.
Seleccionamos con reposici\'on una muestra $\left(\tilde{\boldsymbol{x}}\right)$
de tama\~no $n$. Con este m\'etodo de selecci\'on cada unidad puede aparecer
en la muestra de cero a $n$ veces. Definimos para cada unidad $u_{i}$
la variable aleatoria de apoyo $e_{i}$ como el n\'umero de veces que
la unidad $i$-\'esima, $u_{i}$ pertenece a la muestra de tama\~no $n$.
Estas variables aleatorias se distribuyen seg\'un una binomial de tama\~no
$n$, y probabilidad $P_{i}$, siendo $P_{i}$ la probabilidad que
tiene la unidad $i$-\'esima de entrar en la muestra en cada extracci\'on.
Se tiene que cumpir que $\sum_{i=1}^{N}P_{i}=1$. Por tanto, 
\[
\begin{array}{c}
E\left(e_{i}\right)=nP_{i}\\
Var\left(e_{i}\right)=nP_{i}\left(1-P_{i}\right)
\end{array}
\]


Sean $t_{1},t_{2},\ldots,t_{N}$ el n\'umero de veces que aparece cada
unidad en la muestra. Por tanto, $\sum_{i=1}^{N}t_{i}=n$. La distribuci\'on
de probabilidad para la muestra ser\'a en este caso una multinomial:
\[
P\left(\tilde{\boldsymbol{x}}\right)=\dfrac{n!}{t_{1}!t_{2}!\cdots t_{N}!}P_{1}^{t_{1}}P_{2}^{t_{2}}\cdots P_{N}^{t_{n}}
\]


Su funci\'on generatriz de momentos es:
\[
\begin{array}{c}
g_{\left(e_{1},\ldots,e_{N}\right)}\left(\theta_{1},\ldots,\theta_{N}\right)=E\left(e^{\boldsymbol{\theta}^{\prime}\boldsymbol{e}}\right)=E\left(e^{\theta_{1}e_{1}+\cdots+\theta_{N}e_{N}}\right)=\sum_{t_{1}+t_{2}+\cdots+t_{N}=n}e^{\theta_{1}t_{1}+\cdots+\theta_{N}t_{N}}\dfrac{n!}{t_{1}!t_{2}!\cdots t_{N}!}P_{1}^{t_{1}}P_{2}^{t_{2}}\cdots P_{N}^{t_{n}}=\\
=\sum_{t_{1}+t_{2}+\cdots+t_{N}=n}\left(e^{\theta_{1}}P_{1}\right)^{t_{1}}\cdots\left(e^{\theta_{N}}P_{N}\right)^{t_{N}}\dfrac{n!}{t_{1}!t_{2}!\cdots t_{N}!}=\left(\sum_{i=1}^{N}P_{i}e^{\theta_{i}}\right)^{n}
\end{array}
\]
 y por tanto: $E\left(e_{i}e_{j}\right)=\dfrac{\partial^{2}g\left(0,\ldots,0\right)}{\partial\theta_{i}\partial\theta_{j}}=n\left(n-1\right)P_{i}P_{j}$.
$Cov\left(e_{i},e_{j}\right)=E\left(e_{i}e_{j}\right)-E\left(e_{i}\right)E\left(e_{j}\right)=-nP_{i}P_{j}$,
y as\'i hemos definido el vector esperanza matem\'atica y la matriz de
covarianzas para nuestra variable multinomial.


\section{Comparaci\'on con el muestreo no probabil\'istico: Muestreo por cuotas.}

El muestreo no probabil\'istico es aquel en el que la selecci\'on de la
muestra no est\'a sometida a criterios probabil\'isticos, y por tanto
no se conoce la probabilidad de cada unidad de estar presente en la
muestra. Por tanto, no conoceremos la distribuci\'on de probabilidad
de los estimadores ni podremos calcular los errores lo que nos impedir\'a
evaluar objetivamente los resultados y su calidad. Algunos tipod e
muestreo no probabil\'istico son:
\begin{description}
\item [{Muestreo~opin\'atico:}] Se eligen unidades que se creen que son
especialmente representativas de la poblaci\'on que se quiere investigar.
Con base en la informaci\'on recabada en esas unidades se hacen estimaciones
sobre las car\'acter\'isticas de la poblaci\'on.
\item [{Muestreo~aplicando~criterio:}] La elecci\'on de los componentes
de la muestra se deja a criterio del entrevistador.
\item [{Muestreo~por~cuotas:}] Los entrevistadores tienen libertad de
elegir la muestra, siempre que esta se omponga de un determinado n\'umero
de individuos seg\'un una o varias caracter\'isticas, por ejemplo, determinado
n\'umero de mujers y de hombres. El dise\~no de la encuesta sigue los
principios del muestreo probabil\'istico hasta llegar al momento de
selecci\'on de la muestra. En esta etapa se da a cada encuestador un
n\'umero de encuestas a un deternimado grupo de unidades. El margen
de maniobra del entrevistador puede introducir sesgos en el proceso
de selecci\'on, y el desconocimiento de las probabilidades de selecci\'on
impide evitar errores debido a ponderaciones incorrectas en el proceso
de estimaci\'on, ni podemos calcular los errores debidos al muestreo.\selectlanguage{spanish}%
\end{description}

