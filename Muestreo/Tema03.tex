
\chapter{Muestreo con probabilidades desiguales. Estimadores lineales. Varianza
de los estimadores y sus estimaciones. Probabilidades \'optimas de selecci\'on.
M\'etodos de selecci\'on con reposici\'on y sin reposici\'on y probabilidades
proporcionales al tama\~no.}


\section{Muestreo con probabilidades desiguales.}

Cuando la probabilidad que tiene cualquier unidad de la poblaci\'on
de ser elegida para la muestra no es la misma para todas la unidades
decimos que estamos ante un m\'etodo de muestreo con probabilidades
desiguales. Habr\'a que distinguir entre muestreo sin reposici\'on y con
reposici\'on, y en los casos en que el orden de colocaci\'on de los elementos
intervenga o no sea as\'i.


\subsection{Muestreo sin reposici\'on.}

Como norma general, no se tiene en cuenta el orden de colocaci\'on de
los elementos en la muestra, es decir, muestras con los mismos elementos
extra\'idos en distinto orden son iguales. En este caso, el n\'umero de
muestras de tama\~no $n$ de un espacio muestral con $N$ unidades ser\'a
el de las combinaciones sin repetici\'on de $N$ elementos tomados de
$n$ en $n$, es decir $C_{N,n}=\dbinom{N}{n}=\dfrac{N!}{n!\left(N-n\right)!}$.
Si consideramos que dos muestras con los mismos elementos ordenados
de distinta forma son distintas el n\'umero de muestras de tama\~no $n$
de un espacio muestral con $N$ unidades ser\'a el de las variaciones
sin repetici\'on de $N$ elementos tomados de $n$ en $n$, es decir
$V_{N,n}=\dbinom{N}{n}n!=\dfrac{N!}{\left(N-n\right)!}$.

Consideremos una poblaci\'on de tama\~no $N$, de unidades $\left\{ u_{1},u_{2},\ldots,u_{N}\right\} $.
Seleccionamos sin reposici\'on una muestra $\left(\widetilde{\boldsymbol{x}}\right)$
de tama\~no $n$. Si definimos la variable aleatoria $e_{i}$ como el
n\'umero de veces que la unidad $u_{i}$ aparece en la muestra, esta
variable puede tomar valores entre $0$ y $1$ para cada unidad, y
sigue una distribuci\'on de Bernouilli de par\'ametro $p=\pi_{i}$, es
decir: 
\[
e_{i}=\begin{cases}
1 & u_{i}\in\left(\widetilde{\boldsymbol{x}}\right)\\
0 & u_{i}\notin\left(\widetilde{\boldsymbol{x}}\right)
\end{cases}P\left(e_{i}=1\right)=\pi_{i};\,P\left(e_{i}=0\right)=1-\pi_{i}
\]


Por tanto,$E\left[e_{i}\right]=\pi_{i}$, $E\left[e_{i}^{2}\right]=\pi_{i}$,
$Var\left(e_{i}\right)=\pi_{i}\left(1-\pi_{i}\right)$. Si para cada
$i,j=1,2,\ldots,N$ con $i\neq j$ consideramos la variable aleatoria
producto $e_{i}e_{j}$que estar\'a definida:

\[
e_{i}e_{j}=\begin{cases}
1 & \left(u_{i},u_{j}\right)\in\left(\widetilde{\boldsymbol{x}}\right)\\
0 & \left(u_{i},u_{j}\right)\notin\left(\widetilde{\boldsymbol{x}}\right)
\end{cases}P\left(e_{i}e_{j}=1\right)=\pi_{ij};\,P\left(e_{i}e_{j}=0\right)=1-\pi_{ij}
\]


Por tanto, $E\left[e_{i}e_{j}\right]=\pi_{ij}$; $Cov\left(e_{i},e_{j}\right)=E\left(e_{i}e_{j}\right)-E\left(e_{i}\right)E\left(e_{j}\right)=\pi_{ij}-\pi_{i}\pi_{j}$.

Propiedades de las probabilidades:
\begin{enumerate}
\item $\sum_{i=1}^{N}\pi_{i}=n$, ya que $\sum_{i=1}^{N}\pi_{i}=\sum_{i=1}^{N}E\left(e_{i}\right)=E\left(\sum_{i=1}^{N}e_{i}\right)=E\left(n\right)=n$.
\item $\sum_{i=1,i\neq j}^{N}\pi_{i}=n-\pi_{j}$, ya que $\sum_{i=1}^{N}\pi_{i}=\sum_{i=1,i\neq j}^{N}\pi_{i}+\pi_{j}=n$.
\item $\sum_{i=1,i\neq j}^{N}\pi_{ij}=\left(n-1\right)\pi_{j}$, ya que
$\sum_{i=1,i\neq j}^{N}\pi_{ij}=\sum_{i=1,i\neq j}^{N}E\left(e_{i}e_{j}\right)=E\left(\sum_{i=1,i\neq j}^{N}e_{i}e_{j}\right)=E\left(e_{j}\sum_{i=1,i\neq j}^{N}e_{i}\right)=E\left(e_{j}\sum_{i=1}^{N}e_{i}-e_{j}^{2}\right)=n\pi_{j}-\pi_{j}$.
\item $\sum_{i=1,i\neq j}^{N}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)=-\pi_{j}\left(1-\pi_{j}\right)$,
ya que $\sum_{i=1,i\neq j}^{N}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)=\sum_{i=1,i\neq j}^{N}\pi_{ij}-\sum_{i=1,i\neq j}^{N}\pi_{i}\pi_{j}=\left(n-1\right)\pi_{j}-\pi_{j}\sum_{i=1,i\neq j}^{N}\pi_{i}=$
$=\left(n-1\right)\pi_{j}-\pi_{j}\left(n-\pi_{j}\right)=n\pi_{j}-\pi_{j}-n\pi_{j}+\pi_{j}^{2}=-\pi_{j}\left(1-\pi_{j}\right)$.
\end{enumerate}

\subsection{Muestreo con reposici\'on.}

Como norma general, no se tiene en cuenta el orden de colocaci\'on de
los elementos en la muestra, es decir, muestras con los mismos elementos
extra\'idos en distinto orden son iguales. En este caso, el n\'umero de
muestras de tama\~no $n$ de un espacio muestral con $N$ unidades ser\'a
el de las combinaciones con repetici\'on de $N$ elementos tomados de
$n$ en $n$, es decir $CR_{N,n}=\dbinom{N-n+1}{n}=\dfrac{\left(N+n-1\right)!}{n!\left(N-1\right)!}$.
Si consideramos que dos muestras con los mismos elementos ordenados
de distinta forma son distintas el n\'umero de muestras de tama\~no $n$
de un espacio muestral con $N$ unidades ser\'a el de las variaciones
con repetici\'on de $N$ elementos tomados de $n$ en $n$, es decir
$VR_{N,n}=N^{n}$.

Consideremos una poblaci\'on de tama\~no $N$, de unidades $\left\{ u_{1},u_{2},\ldots,u_{N}\right\} $.
Seleccionamos con reposici\'on una muestra $\left(\widetilde{\boldsymbol{x}}\right)$
de tama\~no $n$. Si definimos la variable aleatoria $e_{i}$ como el
n\'umero de veces que la unidad $u_{i}$ aparece en la muestra, esta
variable puede tomar valores entre $0$ y $n$ para cada unidad, y
sigue una distribuci\'on de binomial de par\'ametros $n$ y $P_{i}$,
siendo $P_{i}$ la probabilidad de selecci\'on de la unidad $i$-\'esima
en cada extracci\'on (probabilidad unitaria de selecci\'on). Por tanto,
$E\left[e_{i}\right]=nP_{i}$, $E\left[e_{i}^{2}\right]=n^{2}P_{i}^{2}+nP_{i}\left(1-P_{i}\right)$,
$Var\left(e_{i}\right)=nP_{i}\left(1-P_{i}\right)$. La probabilidad
de una muestra cualquiera seguir\'a una distribuci\'on multinomial, ya
que cada unidad $u_{i}$ puede seleccionarse $t_{i}$ veces, con $\sum_{i=1}^{N}t_{i}=n$,
por tanto:

\[
P\left(\widetilde{\boldsymbol{x}}\right)=P\left(e_{1}=t_{1},e_{1}=t_{1},\ldots,e_{N}=t_{N}\right)=\dfrac{n!}{t_{1}!t_{1}!\cdots t_{N}!}P_{1}^{t_{1}}P_{2}^{t_{2}}\cdots P_{N}^{t_{N}}
\]


Calculamos su funci\'on generatriz de momentos:

\[
g_{e_{1},e_{2},\ldots,e_{N}}\left(\theta_{1},\theta_{2},\ldots,\theta_{N}\right)=E\left(e^{\theta_{1}e_{1}+\theta_{2}e_{2}+\cdots+\theta_{N}e_{N}}\right)=\left[P_{1}e^{\theta_{1}}+P_{2}e^{\theta_{2}}+\cdots+P_{N}e^{\theta_{N}}\right]^{n}
\]


A partir de esta funci\'on calculamos $E\left(e_{i}e_{j}\right)=\dfrac{\partial^{2}g\left(0,\ldots,0\right)}{\partial\theta_{i}\partial\theta_{j}}=n\left(n-1\right)P_{i}P_{j}$.
$Cov\left(e_{i},e_{j}\right)=E\left(e_{i}e_{j}\right)-E\left(e_{i}\right)E\left(e_{j}\right)=-nP_{i}P_{j}$,
y as\'i hemos definido el vector esperanza matem\'atica y la matriz de
covarianzas para nuestra variable multinomial.


\section{Estimadores lineales insesgados.}

Supongamos que tenemos una poblaci\'on de tama\~no $N$, para la que hemos
definido una caracter\'istica $X_{i}$ que toma el valor $X_{i}$ en
cada unidad $u_{i}$. Supongamos que tenemos un par\'ametro poblacional,
funci\'on de los $N$ valores de las $X_{i}$, que es el que queremos
estimar. En general este par\'ametro se puede expresar como una suma
de elementos $Y_{i}$, que son funci\'on de los valores que la caracter\'istica
$X_{i}$ presenta, $\theta=\sum_{i=1}^{N}Y_{i}$ donde $Y_{i}=Y\left(X_{i}\right)$.
Por ejemplo:
\begin{description}
\item [{Total~poblacional:}] $X=\theta\left(X_{1},\ldots,X_{N}\right)=\sum_{i=1}^{N}X_{i}$,
donde $Y_{i}=X_{i}$.
\item [{Media~poblacional:}] $X=\theta\left(X_{1},\ldots,X_{N}\right)=\dfrac{1}{N}\sum_{i=1}^{N}X_{i}$,
donde $Y_{i}=\dfrac{X_{i}}{N}$.
\item [{Total~de~clase:}] $A=\theta\left(A_{1},\ldots,A_{N}\right)=\sum_{i=1}^{N}A_{i}$,
donde $Y_{i}=A_{i}$. Donde $A_{i}$ se usa para caracter\'isticas cualitativas
dicot\'omicas: $A_{i}=1$ si $u_{1}$ presenta la caracter\'istica, y
$A_{i}=0$ si $u_{1}$ no presenta la caracter\'istica.
\item [{Proporci\'on~de~clase:}] $A=\theta\left(A_{1},\ldots,A_{N}\right)=\dfrac{1}{N}\sum_{i=1}^{N}A_{i}$,
donde $Y_{i}=\dfrac{A_{i}}{N}$.
\end{description}
En general las mejores propiedades suelen presentarlas los estimadores
lineales de la forma $\hat{\theta}=\sum_{i=1}^{n}w_{i}Y_{i}$.
\begin{itemize}
\item Todas las mediciones de la variable de estudio que aparecen en la
muestra intervienen en el estimador.
\item La importancia de la aportaci\'on al estimador de cada unidad muestral
puede controlarse mediante su coeficiente.
\item Cuando $w_{i}=1$ todas las unidades muestrales intervienen con la
misma importancia en el estimador.
\item Cuando las unidades de la muestra son compuestas, el valor de $w_{i}$
puede regular la importancia de cada unidad compuesta asoci\'andola
con su tama\~no o con el n\'umero de unidades elementales que contiene.
\item Los coeficientes pueden depender del tama\~no de las unidades muestrales,
de su orden en la muestra o de las probabilidades que tienen de pertenecer
a la muestra.
\item Las funciones lineales son las m\'as sencillas de manejar matem\'aticamente.
\end{itemize}

\subsection{Muestreo sin reposici\'on.}

Queremos que el estimador sea insesgado, es decir, que su esperanza
sea igual al par\'ametro que queremos estimar. Veamos cual ha de ser
el valor de los coeficientes para que esto ocurra.

\[
E\left(\hat{\theta}\right)=E\left(\sum_{i=1}^{n}w_{i}Y_{i}\right)=E\left(\sum_{i=1}^{N}w_{i}Y_{i}e_{i}\right)=\sum_{i=1}^{N}w_{i}Y_{i}E\left(e_{i}\right)
\]


Ya que $e_{i}=1$ si $u_{i}$ pertenece a la muestra y$e_{i}=0$ si
$u_{i}$ no pertenece a la muestra. Como $\theta=\sum_{i=1}^{N}Y_{i}$,
entonces se tiene que cumplir que $w_{i}E\left(e_{i}\right)=1$ y
por tanto, $w_{i}=\dfrac{1}{E\left(e_{i}\right)}$. Para muestreo
sin reposici\'on $E\left(e_{i}\right)=\pi_{i}$ y por tanto$w_{i}=\dfrac{1}{\pi_{i}}$.
Con esto podemos construir los estimadores, obteniendo el estimador
de Horvitz y Thompson para los distintos par\'ametros poblacionales:
\begin{description}
\item [{Total~poblacional:}] Como $Y_{i}=X_{i}$, $\hat{X}=\sum_{i=1}^{n}w_{i}Y_{i}=\sum_{i=1}^{n}\dfrac{X_{i}}{\pi_{i}}$,
$\hat{X}_{HT}=\sum_{i=1}^{n}\dfrac{X_{i}}{\pi_{i}}$.
\item [{Media~poblacional:}] Como $Y_{i}=\dfrac{X_{i}}{N}$, $\hat{\bar{X}}=\sum_{i=1}^{n}w_{i}Y_{i}=\sum_{i=1}^{n}\dfrac{1}{\pi_{i}}\dfrac{X_{i}}{N}=\dfrac{1}{N}\sum_{i=1}^{n}\dfrac{X_{i}}{\pi_{i}}$,
$\hat{\bar{X}}_{HT}=\dfrac{1}{N}\sum_{i=1}^{n}\dfrac{X_{i}}{\pi_{i}}$.
\item [{Total~de~clase:}] Como $Y_{i}=A_{i}$, $\hat{A}=\sum_{i=1}^{n}w_{i}Y_{i}=\sum_{i=1}^{n}\dfrac{1}{\pi_{i}}A_{i}$,
$\hat{A}_{HT}=\sum_{i=1}^{n}\dfrac{A_{i}}{\pi_{i}}$.
\item [{Proporci\'on~de~clase:}] Como $Y_{i}=\dfrac{A_{i}}{N}$, $\hat{P}=\sum_{i=1}^{n}w_{i}Y_{i}=\sum_{i=1}^{n}\dfrac{1}{\pi_{i}}\dfrac{A_{i}}{N}$,
$\hat{P}_{HT}=\dfrac{1}{N}\sum_{i=1}^{n}\dfrac{A_{i}}{\pi_{i}}$.
\end{description}

\subsection{Muestreo con reposici\'on.}

Queremos que el estimador sea insesgado, es decir, que su esperanza
sea igual al par\'ametro que queremos estimar. Veamos cual ha de ser
el valor de los coeficientes para que esto ocurra.

\[
E\left(\hat{\theta}\right)=E\left(\sum_{i=1}^{n}w_{i}Y_{i}\right)=E\left(\sum_{i=1}^{N}w_{i}Y_{i}e_{i}\right)=\sum_{i=1}^{N}w_{i}Y_{i}E\left(e_{i}\right)
\]


Ya que $e_{i}$ es igual al n\'umero de veces que $u_{i}$ aparece en
la muestra, por tanto $Y_{i}e_{i}$ es lo mismo que sumar $Y_{i}$
tantas veces como la unidad$u_{i}$ aparece en la muestra. Como $\theta=\sum_{i=1}^{N}Y_{i}$,
entonces se tiene que cumplir que $w_{i}E\left(e_{i}\right)=1$ y
por tanto, $w_{i}=\dfrac{1}{E\left(e_{i}\right)}$. Para muestreo
con reposici\'on $E\left(e_{i}\right)=nP_{i}$ y por tanto$w_{i}=\dfrac{1}{nP_{i}}$.
Con esto podemos construir los estimadores, obteniendo el estimador
de Hansen y Hurwitz para los distintos par\'ametros poblacionales:
\begin{description}
\item [{Total~poblacional:}] Como $Y_{i}=X_{i}$, $\hat{X}=\sum_{i=1}^{n}w_{i}Y_{i}=\sum_{i=1}^{n}\dfrac{X_{i}}{nP_{i}}$,
$\hat{X}_{HH}=\sum_{i=1}^{n}\dfrac{X_{i}}{nP_{i}}$.
\item [{Media~poblacional:}] Como $Y_{i}=\dfrac{X_{i}}{N}$, $\hat{\bar{X}}=\sum_{i=1}^{n}w_{i}Y_{i}=\sum_{i=1}^{n}\dfrac{1}{nP_{i}}\dfrac{X_{i}}{N}$,
$\hat{\bar{X}}_{HH}=\dfrac{1}{N}\sum_{i=1}^{n}\dfrac{X_{i}}{nP_{i}}$.
\item [{Total~de~clase:}] Como $Y_{i}=A_{i}$, $\hat{A}=\sum_{i=1}^{n}w_{i}Y_{i}=\sum_{i=1}^{n}\dfrac{A_{i}}{nP_{i}}$,
$\hat{A}_{HH}=\sum_{i=1}^{n}\dfrac{A_{i}}{nP_{i}}$.
\item [{Proporci\'on~de~clase:}] Como $Y_{i}=\dfrac{A_{i}}{N}$, $\hat{P}=\sum_{i=1}^{n}w_{i}Y_{i}=\sum_{i=1}^{n}\dfrac{1}{N}\dfrac{A_{i}}{nP_{i}}$,
$\hat{P}_{HH}=\dfrac{1}{N}\sum_{i=1}^{n}\dfrac{A_{i}}{nP_{i}}$.
\end{description}

\section{Varianzas de los estimadores y sus estimaciones.}

Es importante conocer la expresi\'on de la varianza de estos estimadores,
as\'i como una forma de estimar la misma, para poder evaluar la calidad
de los mismos.


\subsection{Varianza del estimador en muestreo sin reposici\'on.}

\[
\begin{array}{c}
Var\left(\hat{\theta}_{HT}\right)=Var\left(\sum_{i=1}^{n}\dfrac{Y_{i}}{\pi_{i}}\right)=Var\left(\sum_{i=1}^{N}\dfrac{Y_{i}}{\pi_{i}}e_{i}\right)=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}V\left(e_{i}\right)+\sum_{i=1}^{N}\sum_{j\neq i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}Cov\left(e_{i},e_{j}\right)=\\
=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}\pi_{i}\left(1-\pi_{i}\right)+\sum_{i=1}^{N}\sum_{j\neq i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{\pi_{i}}\left(1-\pi_{i}\right)+\sum_{i=1}^{N}\sum_{j\neq i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)
\end{array}
\]



\subsection{Estimaci\'on de las varianzas.}

Para poder obtener una estimaci\'on de la varianza s\'olo podemos utilizar
los datos muestrales, mientras que el valor de la varianza que tenemos
se calcula a partir de la poblaci\'on entera. Necesitamos por tanto
un estimador de la varianza. Utilizaremos la ra\'iz cuadrada de este
estimador como error de muestreo. Un estimador insesgado de la varianza
viene dado por la expresi\'on: 
\[
\begin{array}{c}
\hat{Var}\left(\hat{\theta}_{HT}\right)=\sum_{i=1}^{n}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}\left(1-\pi_{i}\right)+\sum_{i=1}^{n}\sum_{j\neq i}^{n}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\dfrac{\left(\pi_{ij}-\pi_{i}\pi_{j}\right)}{\pi_{ij}}\end{array}
\]


Veamos si es insesgado:

\[
\begin{array}{c}
E\left[\hat{Var}\left(\hat{\theta}_{HT}\right)\right]=E\left[\sum_{i=1}^{n}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}\left(1-\pi_{i}\right)\right]+E\left[\sum_{i=1}^{n}\sum_{j\neq i}^{n}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\dfrac{\left(\pi_{ij}-\pi_{i}\pi_{j}\right)}{\pi_{ij}}\right]\\
=E\left[\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}\left(1-\pi_{i}\right)e_{i}\right]+E\left[\sum_{i=1}^{N}\sum_{j\neq i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\dfrac{\left(\pi_{ij}-\pi_{i}\pi_{j}\right)}{\pi_{ij}}e_{i}e_{j}\right]=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}\left(1-\pi_{i}\right)E\left(e_{i}\right)+\sum_{i=1}^{N}\sum_{j\neq i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\dfrac{\left(\pi_{ij}-\pi_{i}\pi_{j}\right)}{\pi_{ij}}E\left(e_{i}e_{j}\right)=\\
=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}\left(1-\pi_{i}\right)\pi_{i}+\sum_{i=1}^{N}\sum_{j\neq i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\dfrac{\left(\pi_{ij}-\pi_{i}\pi_{j}\right)}{\pi_{ij}}\pi_{ij}=Var\left(\hat{\theta}_{HT}\right)
\end{array}
\]



\subsubsection{Estimador de la varianza de Yates y Grundy.}

Otro estimador insesgado de la varianza viene dado por la expresi\'on:

\[
\hat{Var}\left(\hat{\theta}_{HT}\right)=\sum_{i=1}^{n}\sum_{j\neq i}^{n}\left(\dfrac{Y_{i}}{\pi_{i}}-\dfrac{Y_{j}}{\pi_{j}}\right)^{2}\dfrac{\left(\pi_{i}\pi_{j}-\pi_{ij}\right)}{\pi_{ij}}
\]


Para comprobarlo realizamos las siguientes transformaciones en la
expresi\'on de la varianza, y teniendo en cuenta que $\sum_{i=1,i\neq j}^{N}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)=-\pi_{j}\left(1-\pi_{j}\right)$:

\[
Var\left(\hat{\theta}_{HT}\right)=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}\pi_{i}\left(1-\pi_{i}\right)+2\sum_{i=1}^{N}\sum_{j>i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}\sum_{j=1,j\neq i}^{N}\left(\pi_{i}\pi_{j}-\pi_{ij}\right)+2\sum_{i=1}^{N}\sum_{j>i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\left(\pi_{ij}-\pi_{i}\pi_{j}\right)=
\]
\[
=\sum_{i=1}^{N}\sum_{j=1,j>i}^{N}\left(\dfrac{Y_{i}^{2}}{\pi_{i}^{2}}+\dfrac{Y_{j}^{2}}{\pi_{j}^{2}}\right)\left(\pi_{i}\pi_{j}-\pi_{ij}\right)-2\sum_{i=1}^{N}\sum_{j>i}^{N}\dfrac{Y_{i}}{\pi_{i}}\dfrac{Y_{j}}{\pi_{j}}\left(\pi_{i}\pi_{j}-\pi_{ij}\right)=\sum_{i=1}^{N}\sum_{j=1,j>i}^{N}\left(\dfrac{Y_{i}}{\pi_{i}}-\dfrac{Y_{j}}{\pi_{j}}\right)^{2}\left(\pi_{i}\pi_{j}-\pi_{ij}\right)
\]


Y adem\'as:

\[
E\left[\hat{Var}\left(\hat{\theta}_{HT}\right)\right]=E\left[\sum_{i=1}^{n}\sum_{j\neq i}^{n}\left(\dfrac{Y_{i}}{\pi_{i}}-\dfrac{Y_{j}}{\pi_{j}}\right)^{2}\dfrac{\left(\pi_{i}\pi_{j}-\pi_{ij}\right)}{\pi_{ij}}\right]=E\left[\sum_{i=1}^{N}\sum_{j\neq i}^{N}\left(\dfrac{Y_{i}}{\pi_{i}}-\dfrac{Y_{j}}{\pi_{j}}\right)^{2}\dfrac{\left(\pi_{i}\pi_{j}-\pi_{ij}\right)}{\pi_{ij}}e_{i}e_{j}\right]=\sum_{i=1}^{N}\sum_{j\neq i}^{N}\left(\dfrac{Y_{i}}{\pi_{i}}-\dfrac{Y_{j}}{\pi_{j}}\right)^{2}\dfrac{\left(\pi_{i}\pi_{j}-\pi_{ij}\right)}{\pi_{ij}}E\left(e_{i}e_{j}\right)=
\]


\[
=\sum_{i=1}^{N}\sum_{j=1,j>i}^{N}\left(\dfrac{Y_{i}}{\pi_{i}}-\dfrac{Y_{j}}{\pi_{j}}\right)^{2}\left(\pi_{i}\pi_{j}-\pi_{ij}\right)
\]



\subsection{Varianza del estimador en muestreo con reposici\'on.}

\[
\begin{array}{c}
Var\left(\hat{\theta}_{HH}\right)=Var\left(\sum_{i=1}^{n}\dfrac{Y_{i}}{nP_{i}}\right)=Var\left(\sum_{i=1}^{N}\dfrac{Y_{i}}{nP_{i}}e_{i}\right)=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{n^{2}P_{i}^{2}}V\left(e_{i}\right)+\sum_{i=1}^{N}\sum_{j\neq i}^{N}\dfrac{Y_{i}}{nP_{i}}\dfrac{Y_{j}}{nP_{j}}Cov\left(e_{i},e_{j}\right)=\\
=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{n^{2}P_{i}^{2}}nP_{i}\left(1-P_{i}\right)-\sum_{i=1}^{N}\sum_{j\neq i}^{N}\dfrac{Y_{i}}{nP_{i}}\dfrac{Y_{j}}{nP_{j}}nP_{i}P_{j}=\dfrac{1}{n}\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{P_{i}}-\dfrac{1}{n}\sum_{i=1}^{N}Y_{i}^{2}-\dfrac{1}{n}\sum_{i=1}^{N}\sum_{j\neq i}^{N}Y_{i}Y_{j}\\
\left(\sum_{i=1}^{N}Y_{i}\right)^{2}=\sum_{i=1}^{N}Y_{i}^{2}+\sum_{i=1}^{N}\sum_{j\neq i}^{N}Y_{i}Y_{j}\\
-\sum_{i=1}^{N}\sum_{j\neq i}^{N}Y_{i}Y_{j}=\sum_{i=1}^{N}Y_{i}^{2}-\left(\sum_{i=1}^{N}Y_{i}\right)^{2}=\sum_{i=1}^{N}Y_{i}^{2}-\theta^{2}\\
\dfrac{1}{n}\left[\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{P_{i}}-\sum_{i=1}^{N}Y_{i}^{2}-\sum_{i=1}^{N}\sum_{j\neq i}^{N}Y_{i}Y_{j}\right]=\dfrac{1}{n}\left[\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{P_{i}}-\sum_{i=1}^{N}Y_{i}^{2}+\sum_{i=1}^{N}Y_{i}^{2}-\theta^{2}\right]=\dfrac{1}{n}\left[\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{P_{i}}-\theta^{2}\right]\\
Var\left(\hat{\theta}_{HH}\right)=\dfrac{1}{n}\left[\sum_{i=1}^{N}\left(\dfrac{Y_{i}}{P_{i}}\right)^{2}P_{i}-\theta^{2}\right]
\end{array}
\]


Veamos tambi\'en que:

\[
\begin{array}{c}
\sum_{i=1}^{N}\left(\dfrac{Y_{i}}{P_{i}}-\theta\right)^{2}P_{i}=\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{P_{i}^{2}}P_{i}-2\theta\underbrace{\sum_{i=1}^{N}Y_{i}}_{\theta}+\theta^{2}\underbrace{\sum_{i=1}^{N}P_{i}}_{1}=\sum_{i=1}^{N}\left(\dfrac{Y_{i}}{P_{i}}\right)^{2}P_{i}-\theta^{2}\\
Var\left(\hat{\theta}_{HH}\right)=\dfrac{1}{n}\sum_{i=1}^{N}\left(\dfrac{Y_{i}}{P_{i}}-\theta\right)^{2}P_{i}
\end{array}
\]



\subsection{Estimaci\'on de las varianzas.}

Para poder obtener una estimaci\'on de la varianza s\'olo podemos utilizar
los datos muestrales, mientras que el valor de la varianza que tenemos
se calcula a partir de la poblaci\'on entera. Necesitamos por tanto
un estimador de la varianza. Utilizaremos la ra\'iz cuadrada de este
estimador como error de muestreo. Un estimador insesgado de la varianza
viene dado por la expresi\'on:

\[
\hat{Var}\left(\hat{\theta}_{HH}\right)=\dfrac{1}{n\left(n-1\right)}\left[\sum_{i=1}^{n}\left(\dfrac{Y_{i}}{P_{i}}\right)^{2}-n\hat{\theta}_{HH}^{2}\right]
\]


Veamos si es insesgado:

\[
E\left[\hat{Var}\left(\hat{\theta}_{HH}\right)\right]=\dfrac{1}{n\left(n-1\right)}E\left[\sum_{i=1}^{n}\left(\dfrac{Y_{i}}{P_{i}}\right)^{2}-n\hat{\theta}_{HH}^{2}\right]=\dfrac{1}{n\left(n-1\right)}\left[\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{P_{i}^{2}}E\left(e_{i}\right)-nE\left(\hat{\theta}_{HH}^{2}\right)\right]=
\]


\[
=\dfrac{1}{n\left(n-1\right)}\left[n\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{P_{i}}-n\left(V\left(\hat{\theta}_{HH}\right)+\left[E\left(\hat{\theta}_{HH}\right)\right]^{2}\right)\right]=\dfrac{1}{n-1}\left[\sum_{i=1}^{N}\dfrac{Y_{i}^{2}}{P_{i}}-V\left(\hat{\theta}_{HH}\right)-\theta^{2}\right]=\dfrac{1}{n-1}\left[nV\left(\hat{\theta}_{HH}\right)-V\left(\hat{\theta}_{HH}\right)\right]=V\left(\hat{\theta}_{HH}\right)
\]


Veamos tambi\'en que:

\[
\sum_{i=1}^{n}\left(\dfrac{Y_{i}}{P_{i}}-\hat{\theta}_{HH}\right)^{2}=\sum_{i=1}^{n}\dfrac{Y_{i}^{2}}{P_{i}^{2}}-2\hat{\theta}_{HH}\underbrace{\sum_{i=1}^{n}\dfrac{Y_{i}}{P_{i}}}_{n\hat{\theta}_{HH}}+n\hat{\theta}_{HH}^{2}=\sum_{i=1}^{n}\left(\dfrac{Y_{i}}{P_{i}}\right)^{2}-n\hat{\theta}_{HH}^{2}
\]


\[
\hat{Var}\left(\hat{\theta}_{HH}\right)=\dfrac{1}{n\left(n-1\right)}\sum_{i=1}^{n}\left(\dfrac{Y_{i}}{P_{i}}-\hat{\theta}_{HH}\right)^{2}
\]



\section{Probabilidades \'optimas de selecci\'on.}


\section{M\'etodos de selecci\'on con reposici\'on y sin reposici\'on y probabilidades
proporcionales al tama\~no.}

En muchas ocasiones, especialmente con unidades de muestreo compuestas,
es conveniente asignar a las unidades probabilidades de selecci\'on
que sean proporcionales al tama\~no de la unidad en cuesti\'on. Vamos
a ver distintos esquemas de selecci\'on para estos casos.


\subsection{Estimadores sin reposici\'on.}


\subsubsection{Modelo polinomial o esquema de urna generalizado.}

Sea $M_{i}$ el entero positivo asociado a la unidad compuesta $u_{i}$
que representa su tama\~no. Sea $M=\sum_{i=1}^{N}M_{i}$, es decir,
el tama\~no total de las unidades de la poblaci\'on. Se selecciona la
primera unidad de la muestra con una probabilidad $p_{i}=\dfrac{M_{i}}{M}$.
dado que el muestreo es sin reposici\'on, la siguiente unidad, $j$
se selecciona con una probabilidad $p_{j}=\dfrac{M_{j}}{M-M_{i}}$,
y as\'i sucesivamente hasta seleccionar la muestra completa.

Este modelo equivale a tener una urna con $M_{i}$ bolas por cada
unidad, y seleccionar en cada extracci\'on una bola al azar incorporando
a la muestra la unidad a la que est\'a asociada, y una vez seleccionada
retirar de la urna las $M_{i}$ bolas asociadas a la unidad. Por eso
a este esquema se le llama esquema de urna generalizado. Como $\sum_{i=1}^{N}P_{i}=\sum_{i=1}^{N}\dfrac{M_{i}}{M}=\dfrac{\sum_{i=1}^{N}M_{i}}{M}=1$,
el modelo est\'a bien definido.

Veamos las probabilidades de que una unidad pertenezca a la muestra:

\[
P\left(u_{i}=u_{1}\right)=\dfrac{M_{i}}{M}
\]


\[
P\left(u_{i}=u_{2}\cap u_{i}\neq u_{1}\right)=P\left(u_{i}=u_{2}/u_{i}\neq u_{1}\right)P\left(u_{i}\neq u_{1}\right)=\sum_{j\neq i}\dfrac{M_{i}}{M-M_{j}}\dfrac{M_{j}}{M}=\dfrac{M_{i}}{M}\sum_{j\neq i}\dfrac{M_{j}}{M-M_{j}}=\dfrac{M_{i}}{M}\sum_{j\neq i}\dfrac{M_{j}/M}{M/M-M_{j}/M}=P_{i}\sum_{j\neq i}\dfrac{P_{j}}{1+P_{j}}
\]


\[
P\left(u_{i}=u_{3}\cap u_{i}\neq u_{2}\cap u_{i}\neq u_{1}\right)=P\left(u_{i}=u_{3}/u_{i}\neq u_{2}\cap u_{i}\neq u_{1}\right)P\left(u_{i}\neq u_{2}/u_{i}\neq u_{1}\right)P\left(u_{i}\neq u_{1}\right)=
\]


\[
=\sum_{j\neq i}\sum_{k\neq i,k\neq j}P\left(u_{i}=u_{3}/u_{j}=u_{2}\cap u_{k}=u_{1}\right)P\left(u_{j}=u_{2}/u_{k}=u_{1}\right)P\left(u_{k}=u_{1}\right)=\sum_{j\neq i}\sum_{k\neq i,k\neq j}\dfrac{M_{i}}{M-M_{j}-M_{k}}\dfrac{M_{j}}{M-M_{k}}\dfrac{M_{k}}{M}=
\]


\[
=\dfrac{M_{i}}{M}\sum_{j\neq i}\sum_{k\neq i,k\neq j}\dfrac{M_{i}}{M-M_{j}-M_{k}}\dfrac{M_{j}}{M-M_{k}}\dfrac{M_{k}}{M}=
\]


\[
\vdots
\]


\[
P\left(u_{i}=u_{n}\cap u_{i}\neq u_{n-1}\cap\cdots\cap u_{i}\neq u_{1}\right)=P\left(u_{i}=u_{n}/u_{i}\neq u_{n-1}\cap\cdots\cap u_{i}\neq u_{1}\right)\cdots P\left(u_{i}\neq u_{2}/u_{i}\neq u_{1}\right)P\left(u_{i}\neq u_{1}\right)=
\]


\[
=\dfrac{1}{N-\left(n-1\right)}\dfrac{N-\left(n-1\right)}{N-\left(n-2\right)}\cdots\dfrac{N-2}{N-1}\dfrac{N-1}{N}=\dfrac{1}{N}
\]


\[
\pi_{i}=P\left(u_{i}\in\left(\tilde{x}\right)\right)=P\left(u_{i}=u_{1}\right)+P\left(u_{i}=u_{2}\cap u_{i}\neq u_{1}\right)=P_{i}\left(1+\sum_{j\neq i}\dfrac{P_{j}}{1+P_{j}}\right)
\]


\[
\pi_{ij}=P\left(u_{j}=u_{2}\cap u_{i}=u_{1}\right)+P\left(u_{i}=u_{2}\cap u_{j}=u_{1}\right)=\dfrac{M_{i}}{M-M_{j}}\dfrac{M_{j}}{M}+\dfrac{M_{j}}{M-M_{i}}\dfrac{M_{i}}{M}=P_{i}P_{j}\left(\dfrac{1}{1-P_{i}}+\dfrac{1}{1-P_{j}}\right)
\]



\subsubsection{Modelo de Ikeda.}

Ikeda propuso el m\'etodo de selecci\'on siguiente: la primera unidad
se elige con probabilidades proporcionales al tama\~no, y el resto sin
reposici\'on y con probabilidades iguales. Sea $P_{i}=\dfrac{M_{i}}{M}$
la probabilidad asignada a la unidad $i$, calculemos $\pi_{i}$ y
$\pi_{ij}$. La probabilidad de que la unidad $i$ est\'e presente es
igual a la probabilidad de elegirla la primera m\'as la probabilidad
de no elegirla la primera y s\'i en una de las siguientes elecciones.

\[
\pi_{i}=P_{i}+\left(1-P_{i}\right)\dfrac{n-1}{N-1}=\dfrac{N-n}{N-1}P_{i}+\dfrac{n-1}{N-1}
\]


De modo an\'alogo calculamos $\pi_{ij}$ como la probabilidad de elegir
la unidad $i$ la primera y la $j$ en una de las siguientes elecciones,
m\'as la probabilidad de elegir la unidad $j$ la primera y la $i$
en una de las siguientes elecciones, m\'as la probabilidad de no elegir
ninguna de las dos la primera y elegirlas en una de las siguientes
elecciones.

\[
\pi_{ij}=P_{i}\dfrac{n-1}{N-1}+P_{j}\dfrac{n-1}{N-1}+\left(1-\left(P_{i}+P_{j}\right)\dfrac{n-1}{N-1}\dfrac{n-2}{N-2}\right)=\dfrac{n-1}{N-1}\left[\dfrac{N-n}{N-2}\left(P_{i}+P_{j}\right)+\dfrac{n-2}{N-2}\right]
\]


La probabilidad de una muestra en particular, $s$, es igual a la
probabilidad de obtener la unidad $i$ en la primera selecci\'on por
la probabilidad de las $n-1$ restantes, sumado para las $n$ unidades
que componen la muestras:

\[
P\left(s\right)=\sum_{i=1}^{n}P_{i}\dfrac{1}{\binom{N-1}{n-1}}=\dfrac{1}{\binom{N-1}{n-1}}\sum_{i=1}^{n}\dfrac{M_{i}}{M}=\dfrac{1}{M}\dfrac{1}{\binom{N-1}{n-1}}\sum_{i=1}^{n}M_{i}=k\sum_{i=1}^{n}M_{i}
\]
 es decir, la probabilidad de una muestra es proporcional al tama\~no
de sus unidades sumado.


\subsection{Con reposici\'on.}


\subsubsection{Modelo polinomial.}

Sea $M_{i}$ el entero positivo asociado a la unidad compuesta $u_{i}$
que representa su tama\~no. Sea $M=\sum_{i=1}^{N}M_{i}$, es decir,
el tama\~no total de las unidades de la poblaci\'on. Se seleccionan las
unidades de la muestra con una probabilidad $P_{i}=\dfrac{M_{i}}{M}$.
Este muestreo es con probabilidades proporcionales al tama\~no, $P_{i}=kM_{i}$.
Un m\'etodo pr\'actico para seleccionar muestras con esta configuraci\'on
es definir el intervalo $\left[1,M\right]$ y dividirlo en $N$ subintervalos
$I_{i}$, cada uno de una longitud $M_{i}$. Se elige de forma aleatoria
un n\'umero $\delta$ en el intervalo definido, y se incorpora a la
muestra la unidad $u_{i}$ tal que $\delta\in I_{i}$.


\subsubsection{M\'etodo de Lahiri.}


\subsection{Esquema mixto de selecci\'on de S\'anchez-Crespo y Gabeiras.}

Se considera un esquema de urna en el que cada unidad $u_{i}$ est\'a
representada por $M_{i}$ bolas. Se selecciona una bola al azar, se
incorpora la unidad correspondiente a la muestra y no se reemplaza
la bola seleccionada. As\'i, este modelo tiene probabilidades gradualmente
variables, y se podr\'a extraer la inidad $i$ tantas veces como el
m\'inimo entre su tama\~no y el tama\~no de la muestra.

Es un m\'etodo mixto de selecci\'on: por un lado es sin reposici\'on pues
cuando se retira un representante de la unidad $i$ no se repone,
pero tiene caracter\'isticas del muestreo con reposici\'on, pues una unidad
puede estar presente m\'as de una vez en la muestra.

Definimos la variable aleatoria $e_{i}$ como el n\'umero de veces que
la unidad $i$ est\'a presente en la muestra. Se distribuye seg\'un una
Hipergeom\'etrica de par\'ametros $M$, $n$ y $P_{i}$, siendo $P_{i}$
la probabilidad de elegir la unidad $i$-esima para la muestra.

\[
E\left(e_{i}\right)=nP_{i}\,\,\,Var\left(e_{i}\right)=\dfrac{M-n}{M-1}nP_{i}\left(1-P_{i}\right)
\]


La probabilidad de una muestra de tama\~no $n$ tendr\'a una distribuci\'on
hipergeom\'etrica generalizada. Sea $t_{i}$ el n\'umero de veces que
aparece la unidad $i$-esima en la muestra con $\sum_{i=1}^{N}t_{i}=n$,
y:

\[
P\left(\tilde{x}\right)=p\left(e_{1}=t_{1},e_{2}=t_{2},\ldots,e_{N}=t_{N}\right)=\dfrac{\binom{M_{1}}{t_{1}}\binom{M_{2}}{t_{2}}\cdots\binom{M_{N}}{t_{N}}}{\binom{\sum_{i=1}^{N}M_{i}}{\sum_{i=1}^{N}t_{i}}}=\dfrac{\binom{MP_{1}}{t_{1}}\binom{MP_{2}}{t_{2}}\cdots\binom{MP_{N}}{t_{N}}}{\binom{M}{n}}
\]


\[
E\left(e_{i}e_{j}\right)=MP_{i}P_{j}\dfrac{n\left(n-1\right)}{\left(M-1\right)}\,\,\,Cov\left(e_{i},e_{j}\right)=\dfrac{M-n}{M-1}nP_{i}P_{j}
\]



\subsubsection{Estimador lineal insesgado de S\'anchez-Crespo y Gabeiras.}

Estimamos la caracter\'istica pobacional $\theta=\sum_{i=1}^{N}Y_{i}$.
El estimador lineal ser\'a $\hat{\theta}=\sum_{i=1}^{n}\omega_{i}Y_{i}$,
y para que sea insesgado debe cumplir que $E\left(\hat{\theta}\right)=\theta$.
Por tanto,

\[
E\left(\hat{\theta}\right)=E\left(\sum_{i=1}^{n}\omega_{i}Y_{i}\right)=E\left(\sum_{i=1}^{N}\omega_{i}Y_{i}e_{i}\right)=\sum_{i=1}^{N}\omega_{i}Y_{i}E\left(e_{i}\right)=\sum_{i=1}^{N}\omega_{i}Y_{i}nP_{i}=\sum_{i=1}^{N}Y_{i}
\]


Y por tanto, para ue sea insesgado, $\omega_{i}=\dfrac{1}{nP_{i}}$
y el estimador insesgado de S\'anchez-Crespo y Gabeiras ser\'a $\hat{\theta}_{SCG}=\sum_{i=1}^{n}\dfrac{Y_{i}}{nP_{i}}$
que coincide con la expresi\'on del estimador de Hansen y Hurwitz.

Su varianza ser\'a 
