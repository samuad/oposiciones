\chapter[An\'alisis de la varianza.]{An\'alisis de la varianza. \\
\normalsize An\'alisis de la varianza para una clasificaci\'on simple. Comprobaci\'on de las hip\'otesis iniciales del modelo. Contrastes de comparaciones m\'ultiples: m\'etodo de Tuckey y m\'etodo de Scheff\'e. An\'alisis de la varianza para una clasificaci\'on doble.}

\sectioncol{Introducci\'on.}
El an\'alisis de la varianza es un procedimiento dise\~nado para descomponer la variabilidad de un experimento en componentes que puedan asignarse a causas distintas. Se utiliza cuando tenemos un conjunto de elementos que se dividen en varios grupos diferenciados por un factor. Observamos una caracter\'istica cont\'inua que var\'ia aleatoriamente de esos elementos, y queremos conocer si el factor diferencial afecta al valor medio de la caracter\'istica en estudio.

\sectioncol{An\'alisis de la varianza para una clasificaci\'on simple.}

Supongamos que estamos interesados en estudiar una caracter\'istica $Y$ dentro de una poblaci\'on que se puede dividir en $m$ grupos atendiendo a un factor asociado a sus individuos. Tomamos una muestra aleatoria de tama\~no $N$, y para cada elemento registramos el valor de $Y$ y el grupo al que pertenece, obteniendo una muestra con $n_i$ elementos para el grupo $i$. Queremos averiguar si la media de $Y$ es igual para todos los grupos.

Para ello, suponemos que la media de $Y$ oscila en torno a un valor $\mu$, y que cada grupo produce una variaci\'on en la media de su grupo de $\alpha_i$. Adem\'as, suponemos que la varianza del error aleatorio de observaci\'on, $\sigma^2$, es la misma para toda la poblaci\'on.

Por tanto, formulamos el siguiente modelo:
\[Y_{ij}=\mu+\alpha_i+e_{ij}\;\;\;i=1,\ldots,m,\;j=1,\ldots,n_i\]
donde $\sum_{i=1}^{m}n_i=N$ y $e_ij\sim N(0,\sigma)$.

En este modelo, cada elemento es como sigue:
\begin{itemize}
\item $Y_{ij} =$ valor de la caracter\'istica $Y$ en el individuo $j$ del grupo $i$.
\item $\mu =$ parte del valor medio de la variable com\'un a todos los grupos.
\item $\alpha_i = $ parte del valor medio de la variable espec\'ifico del grupo $i$.
\item $e_ij=$ componentes aleatorios, independientes e id\'enticamente distribu\'idos.
\end{itemize}

Dado que el error normalmente se debe a un conjunto muy grande de factores, cada uno de los cuales influye muy poco en el error final, aplicando el teorema central del l\'imite no es muy descabellado asumir su normalidad.

Es bastante importante que la selecci\'on de los individuos sea aleatoria. De esta forma protegeremos al modelo de fuentes desconocidas de variaci\'on que de otra forma podr\'ian provocar sesgos de selecci\'on y nos proveemos de una base estad\'istica para justificar las hip\'otesis asociadas al modelo.

En el caso de que el factor diferenciador sea, en lugar de una caracter\'istica de los individuos, un tratamiento que se les da tras su elecci\'on, es necesarios realizar la asignaci\'on de forma aleatoria.

Dado que nuestro modelo lo podemos representar como:
\[Y_{ij}=\mu_i+e_{ij}\]

donde $\mu_i=\mu+\alpha_i$ es la media de cada grupo. Por tanto, $Y_{ij}\sim N(\mu_i;\sigma)$. Queremos contrastar la hip\'otesis nula de que todos los grupos son iguales, es decir, $H_0:\mu_1=\mu_2=\cdots=\mu_a$. Si esta hip\'otesis es cierta, se podr\'an considerar nuestras observaciones como una muestra de una \'unica poblaci\'on.

Nuestro modelo depende de $a+1$ par\'ametros: las $a$ medias de cada grupo, y la varianza com\'un a todos los grupos. Para estimar estos par\'ametros utilizaremos el m\'etodo de m\'axima verosimilitud. La funci\'on de densidad de una observaci\'on cualquiera ser\'a:

\[f(y_{ij};\mu_i,\sigma^2)=\dfrac{1}{\sigma\sqrt{2\pi}}e^{-\dfrac{1}{2}\left(\dfrac{y_{ij}-\mu_i}{\sigma}\right)^2}\]

Como por hip\'otesis las observaciones son independientes entre s\'i, la funci\'on de verosimilitud ser\'a el producto de las funciones de densidad, y su logaritmo:
\[\ln{L(\mu_i,\sigma^2)}=-\dfrac{N}{2}\ln{\sigma^2}-\dfrac{N}{2}\ln{2\pi}-\dfrac{1}{2}\sum_{i=1}^{a}\sum_{j=1}^{n_i}\left(\dfrac{y_{ij}-\mu_i}{\sigma}\right)^2\]
Los estimadores de m\'axima verosimilitud de las $\mu_i$ se obtienen a partir de la condici\'on \[\dfrac{\partial\ln{L}}{\partial\mu_i}=0\Leftrightarrow\sum_{j=1}^{n_i}\left(\dfrac{y_{ij}-\mu_i}{\sigma}\right)=0\]

Y por tanto, $\hat{\mu_i}=\dfrac{\sum_{j=1}^{n_i}Y_{ij}}{n_i}=\bar{Y}_{i.}$.

Para obtener el estimador de la varianza, nos basamos en:

\[\dfrac{\partial\ln{L}}{\partial\sigma^2}=0\Leftrightarrow-\dfrac{N}{2\sigma^2}+\dfrac{\sum_{i=1}^{a}\sum_{j=1}^{n_i}\left(y_{ij}-\mu_i\right)^2}{2\sigma^4}=0\]

Y por tanto:
\[\hat{\sigma}^2=\dfrac{\sum_{i=1}^{a}\sum_{j=1}^{n_i}\left(y_{ij}-\mu_i\right)^2}{N}=\dfrac{\sum_{i=1}^{a}\sum_{j=1}^{n_i}\left(Y_{ij}-\bar{Y}_{i.}\right)^2}{N}=\sum_{i=1}^{a}\dfrac{n_i-1}{N}S_i^2\]

Siendo $S_i^2$ la cuasivarianza muestral de cada grupo. As\'i, el estimador de la varianza es la media ponderada de la cuasivarianza muestral de cada grupo.

Los residuos del modelo, definidos como $e_{ij}=Y_{ij}-\hat{mu}_{i}$ se pueden interpretar como los estimadores de las perturbaciones. Estos residuos no son independientes, ya que est\'an sometidos a las restricciones que se derivan de la estimaci\'on de los par\'ametros, es decir, ya que $\hat{\mu_i}=\dfrac{\sum_{j=1}^{n_i}Y_{ij}}{n_i}=\bar{Y}_{i.}$, $\sum_{j=1}^{n_i}e_{ij}=\sum_{j=1}^{n_i}Y_{ij}-n_i\bar{Y}_{i.}=0$. Por tanto, existen $N-a$ residuos no determinados a priori. A este n\'umero lo llamamos grados de libertad de los residuos.

Como $E[\bar{Y}_{i.}]=\dfrac{\sum_{j=1}^{n_i}E[Y_{ij}]}{n_i}=\mu_i$, $V[\bar{Y}_{i.}]=\dfrac{\sum_{j=1}^{n_i}V[Y_{ij}]}{n_i^2}=\dfrac{\sigma^2}{n_i}$, tenemos que $[\bar{Y}_{i.}\sim N\left(\mu_i;\dfrac{\sigma^2}{n_i}\right)$.

Como $\sum_{i=1}^{a}\sum_{j=1}^{n_i}e_{ij}^2=\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y}_{i.})^2=N\hat{\sigma}^2$,
\[\dfrac{N\hat{\sigma}^2}{\sigma^2}=\sum_{i=1}^{a}\sum_{j=1}^{n_i}\left(\dfrac{y_{ij}-\bar{Y}_{i.}}{\sigma}\right)^2\sim\sum_{i=1}^{a}\chi^2_{n_i-1}=\chi^2_{N-a}\]

Gracias a la propiedad reproductiva de la $\chi^2$, y a que las observaciones son independientes.

Como $E[\dfrac{N\hat{\sigma}^2}{\sigma^2}]=N-a$, $E[\hat{\sigma}^2=\dfrac{N-a}{N}\sigma^2$, por lo que el estimador es sesgado. Un estimador insesgado ser\'a lo que llamamos varianza residual: $\hat{S}_R^2=\dfrac{N}{N-a}\hat{\sigma}^2=\dfrac{\sum_{i=1}^{a}\sum_{j=1}^{n_i}e_{ij}^2}{N-a}$, y su distribuci\'on ser\'a proporcional a una $\chi^2$ con $N-a$ grados de libertad.

Queremos contrastar la hip\'otesis nula de uqe los grupos son estad\'isticamente id\'enticos. Dado que por hip\'otesis su varianza es la misma, esto implicar\'a que a partir de nuestras $a$ muestras aleatorias simples tendremos que contrastar la siguiente hip\'otesis nula:
\[H_0:\alpha_1=\alpha_2=\cdots=\alpha_a\]
frente a la hip\'otesis alternativa de que no todas las medias espec\'ificas son iguales.
O, de forma equivalente:
\[H_0:\mu_1=\mu_2=\cdots=\mu_a\]
frente a $H_1:$ al menos dos $\mu_i$ son distintas.

Para facilitar los c\'alculos vamos a introducir la siguiente notaci\'on:
\begin{itemize}
\item \textbf{Tama\~no muestral global:} $N=\sum_{i=1}^an_i$.
\item \textbf{Media muestral $i$-\'esima:} $\bar{Y}_{i.}=\dfrac{1}{n_i}\sum_{j=1}^{n_i}Y_{ij}$.
\item \textbf{Total muestral $i$-\'esimo:} $Y_{i.}=n_i\bar{Y}_{i.}=\sum_{j=1}^{n_i}Y_{ij}$.
\item \textbf{Media muestral global:} $\bar{Y}_{..}=\dfrac{1}{N}\sum_{i=1}^a\sum_{j=1}^{n_i}Y_{ij}=\sum_{i=1}^a\dfrac{n_i}{N}\bar{Y}_{i.}$.
\item \textbf{Total muestral global:} $Y_{..}=N\bar{Y}_{..}=\sum_{i=1}^a\sum_{j=1}^{n_i}Y_{ij}=\sum_{i=1}^an_i\bar{Y}_{i.}$.
\item \textbf{Cuasivarianza muestral $i$-\'esima:} $S_i^2=\dfrac{1}{n_i-1}\sum_{j=1}^{n_i}\left(Y_{ij}-\bar{Y}_{i.}\right)^2$.
\item \textbf{Suma de cuadrados de las observaciones a sus medias grupales:} $SCE=\sum_{i=1}^a(n_i-1)S_i^2=\sum_{i=1}^a\sum_{j=1}^{n_i}\left(Y_{ij}-\bar{Y}_{i.}\right)^2$.

Esta cantidad es una medida de las variaciones dentro de cada grupo debida a la aleatoriedad, es decir, no explicada por el modelo. Coincide con la suma de los residuos.

\item \textbf{Suma de cuadrados de las medias grupales a la media total:} $SCTR=\sum_{i=1}^an_i\left(\bar{Y}_{i.}-\bar{Y}_{..}\right)^2$.

Esta cantidad es una medida de la variabilidad de los datos debida a su pertenencia a grupos distintos, es decir, explicada por el modelo.
\item \textbf{Suma de cuadrados total:} $SCT=\sum_{i=1}^a\sum_{j=1}^{n_i}\left(Y_{ij}-\bar{Y}_{..}\right)^2$.

Esta cantidad es una medida de la variabilidad total de los datos.

\item \textbf{Media de cuadrados del error:} $MCE=\dfrac{SCE}{N-a}$.
\item \textbf{Media de cuadrados de los tratamientos:} $MCTR=\dfrac{SCTR}{a-1}$.

\end{itemize}

Podemos demostrar varios resultados:

\textbf{Resultado 1:} $SCT=SCE+SCTR$:
\begin{align*}
SCT=\sum_{i=1}^a\sum_{j=1}^{n_i}\left(Y_{ij}-\bar{Y}_{..}\right)^2=&\sum_{i=1}^a\sum_{j=1}^{n_i}\left(Y_{ij}-\bar{Y}_{i.}+\bar{Y}_{i.}-\bar{Y}_{..}\right)^2=\\
=\sum_{i=1}^a\sum_{j=1}^{n_i}\left(Y_{ij}-\bar{Y}_{i.}\right)^2+\sum_{i=1}^a\sum_{j=1}^{n_i}\left(\bar{Y}_{i.}-\bar{Y}_{..}\right)^2+&2\sum_{i=1}^a\left(\bar{Y}_{i.}-\bar{Y}_{..}\right)\sum_{j=1}^{n_i}\left(Y_{ij}-\bar{Y}_{i.}\right)=SCTR+SCE
\end{align*}

\textbf{Resultado 2:} Se verifican las siguientes igualdades:
\begin{itemize}
\item $E[SCE]=(N-a)\sigma^2$.
\[E[SCE]=\sum_{i=1}^aE[(n_i-1)S_i^2]=\sigma^2\sum_{i=1}^aE[\dfrac{(n_i-1)S_i^2}{\sigma^2}]=\sigma^2\sum_{i=1}^aE[\chi_{n_i-1}^2]=\sigma^2\sum_{i=1}^a(n_i-1)=(N-a)\sigma^2\]
\item $E[SCTR]=(a-1)\sigma^2+\sum_{i=1}^an_i(\alpha_i-\bar{\alpha})^2$, donde $\bar{\alpha}=\sum_{i=1}^a\dfrac{n_i}{N}\alpha_i$.
\begin{align}
E[SCTR]=E[\sum_{i=1}^an_i(\bar{Y}_{i.}-\bar{Y}_{..})^2]=&\sum_{i=1}^aE[n_i\bar{Y}_{i.}^2]+\sum_{i=1}^an_iE[\bar{Y}_{..}^2]-2E[\sum_{i=1}^an_E[\bar{Y}_{i.}\bar{Y}_{..}]=\sum_{i=1}^aE[n_i\bar{Y}_{i.}^2]+NE[\bar{Y}_{..}^2]-2NE[\bar{Y}_{..}^2]=\\
=\sum_{i=1}^an_iE[\bar{Y}_{i.}^2]-NE[\bar{Y}_{..}^2]=&\sum_{i=1}^an_i(\dfrac{\sigma^2}{n_i}+(\mu+\alpha_i)^2)-N\left[\dfrac{\sigma^2}{N}+\left(\sum_{i=1}^a\dfrac{n_i}{N}(\mu+\alpha_i)\right)^2\right]=(a-1)\sigma^2+\sum_{i=1}^an_i(\mu+\alpha_i)^2-N\left(\sum_{i=1}^a\dfrac{n_i}{N}(\mu+\alpha_i)\right)^2
\end{align}
\[E[\bar{Y}_{i.}^2]=V[\bar{Y}_{i.}]+E[\bar{Y}_{i.}]^2=\dfrac{\sigma^2}{n_i}+(\mu+\alpha_i)^2\]
\[E[\bar{Y}_{..}^2]=V[\bar{Y}_{..}]+E[\bar{Y}_{..}]^2=\dfrac{\sigma^2}{N}+\left(\sum_{i=1}^a\dfrac{n_i}{N}(\mu+\alpha_i)\right)^2\]
\item $E[SCT]=(N-1)\sigma^2+\sum_{i=1}^an_i(\alpha_i-\bar{\alpha})^2$. Se deduce de las dos anteriores y de que $SCT=SCE+SCTR$.
\end{itemize}

Como consecuencia de estos resultados, $E[MCE]=\sigma^2$, y por tanto $MCE$ ser\'a un estimador insesgado de la varianza; tambi\'en $E[S_i^2]=\sigma^2$, y tambi\'en son estimadores insesgados, aunque menos precisos por contar con menos observaciones.

Definimos el estad\'istico:
\[F=\dfrac{MCTR}{MCE}=\dfrac{SCTR/(a-1)}{SCE/(N-a)}\]

Si lo consideramos como un estimador del siguiente cociente:
\[R^{*}=\dfrac{E[MCTR]}{E[MCE]}=1+\dfrac{1}{(a-1)\sigma^2}\sum_{i=1}^an_i(\alpha_i-\bar{\alpha})^2\]

Bajo la hip\'otesis nula, $R^{*}=1$ y bajo la hip\'otesis alternativa $R^{*}>1$. Un contraste basado en $F$ deber\'a rechazar la hip\'otesis nula si $F$ es grande. Para conocer la regi\'on cr\'itica de este contraste deberemos averiguar la distribuci\'on de $F$.

\textbf{Resultado 3:} Bajo la hip\'otesis nula, $F\sim F_{a-1;N-a}$

Gracias al teorema de Fisher, sabemos que $\bar{Y}_{i.}$ y $S_i^2$ son independientes. Como $\bar{Y}_{..}=\sum_{i=1}^a\dfrac{n_i}{N}\bar{Y}_{i.}$, tambi\'en ser\'a independiente de las $S_i^2$. Por tanto, $SCE$ y $SCTR$ son independientes.
\[\dfrac{SCE}{\sigma^2}=\sum_{i=1}^a\dfrac{(n_i-1)S_i^2}{\sigma^2}\]

Y como $\dfrac{(n_i-1)S_i^2}{\sigma^2}\sim\chi^2_{n_i-1}$ y la $\chi^2$ es reproductiva, 
$\dfrac{SCE}{\sigma^2}\sim\chi^2_{N-a}$.

\[\dfrac{SCTOT}{\sigma^2}=\dfrac{1}{\sigma^2}\sum_{i=1}^a\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y}_{..})^2=\dfrac{(N-1)S^2}{\sigma^2}\sim\chi^2{N-1}\]

Si se cumple la hip\'otesis nula. Y como $\dfrac{SCTOT}{\sigma^2}=\dfrac{SCE}{\sigma^2}+\dfrac{SCTR}{\sigma^2}$, y por la propiedad reproductiva de la $\chi^2$, $\dfrac{SCTR}{\sigma^2}\sim\chi^2_{a-1}$. As\'i, tenemos que:
\[F=\dfrac{SCTR/\sigma^2(a-1)}{SCE/\sigma^2(N-a)}=\dfrac{\chi^2_{a-1}/(a-1)}{\chi^2_{N-a}/(N-a)\sim F_{a-1;N-a}}\]

Si realizamos el contraste mediante la regi\'on cr\'itica, rechazamos $H_0$ con un nivel de significaci\'on $1-\alpha$ si $F\in RC$, siendo:
\[RC=\{F>F_{a-1,N-a;\alpha}\}, \text{ con } P[F_{a-1,N-a}>F_{a-1,N-a;\alpha}]=\alpha\]
Si realizamos el contraste mediante el p-valor, rechazamos $H_0$ con un nivel de significaci\'on $1-\alpha$ si el $p-$valor es $p\leq\alpha$, siendo:
\[p=P[F_{a-1,N-a}>F]\]

Estos resultados se suelen resumir en la tabla ANOVA.

Se define el \textbf{coeficiente de determinaci\'on}, $R^2$ como:
\[R^2=\dfrac{SCTR}{SCT}\]

Y se utiliza como una medida de la variabilidad explicada por los grupos.

