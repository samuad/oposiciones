\chapter[Contrastes de bondad de ajuste.]{Contrastes de bondad de ajuste. \\
\normalsize Contraste $\chi^2$ de Pearson. Contraste de Kolmogorov-Smirnov. Contrastes de normalidad. Contrastes de independencia. Contraste de homogeneidad.}

\sectioncol{Introducci\'on.}
Hasta ahora los contrastes que hemos visto se han basado en una poblaci\'on para la que se conoce su distribuci\'on de probabilidad y se desconocen uno o varios par\'ametros que definen dicha distribuci\'on. Sin embargo, esto no es lo habitual, y el asignar una distribuci\'on de probabilidad subyacente err\'onea hace que en muchos casos el contraste deje de tener validez, o, de tenerla, disminuya mucho su eficiencia.

Para obviar este problema se han desarrollado las t\'ecnicas no param\'etricas, en las que el conjunto de hip\'otesis de partida se reducen, e incluso desaparecen, con lo que disminuye el riesgo de errores dentro del proceso inferencial debido a una mala formulaci\'on de dichas hip\'otesis.

Dentro de los contrastes no param\'etricos est\'an los de bondad del ajuste. El objetivo de los contrastes de bondad del ajuste es verificar si una muestra proviene de una poblaci\'on con una determinada distribuci\'on de probabilidad. En general la hip\'otesis nula asume que la distribuci\'on propuesta es la correcta. 


\sectioncol{Contraste $\chi^2$ de Pearson.}

Es el m\'as antiguo. Fue introducido por Pearson en 1900. La idea b\'asica consiste en comparar las frecuencias observadas en un histograma con las esperadas para el modelo te\'orico que se contrasta. Es v\'alido para todo tipo de distribuciones.

El contraste tiene dos posibles casos:
\begin{enumerate}
\item \textit{Hip\'otesis nula simple:} Conocemos los par\'ametros de la distribuci\'on especificada en la hip\'otesis nula.
\item \textit{Hip\'otesis nula compuesta:} No conocemos los par\'ametros de la distribuci\'on especificada en la hip\'otesis nula. As\'i, solo contrastaremos si los datos provienen de la familia de distribuciones especificada.
\end{enumerate}

\subsectioncol{Hip\'otesis nula simple.}

Supongamos que tenemos una poblaci\'on para la que queremos contrastar que su distribuci\'on de probabilidad es $f(x;\theta)$, con $\theta$ conocido.

Tomamos una muestra aleatoria simple de tama\~no $n$. Dividimos el rango de variaci\'on de la variable poblacional, $A$ en $r$ clases, $A_1,\ldots, A_r$, de manera que cumplan:

\begin{itemize}
\item $A_i\cap A_j=\emptyset$, siempre que $i\neq j$.
\item $\bigcup_{i=1}^rA_i=A$.
\end{itemize}

Es decir, las clases son disjuntas entre s\'i y ocupan todo el rango de variaci\'on de la varaible poblacional.

Sean $n_1,\ldots,n_r$ el n\'umero de elementos de la muestra que pertenece a cada clase. Por tanto, $\sum_{i=1}^rn_i=n$.

Bajo el supuesto de que la hip\'otesis nula es cierta, podemos calcular la probabilidad de que un elemento de la muestra pertenezca a cada clase. Es decir, $p(A_i)=p_i$, y $p_i$ ser\'a la probabilidad de que un elemento de la muestra pertenezca a la clase $A_i$.

Como la muestra es aleatoria simple, las observaciones en ella son independientes, por tanto, la probabilidad de que aparezcan $n_1$ elementos en la clase $A_1$, $n_2$ elementos en la clase $A_2$, ... $n_k$ elementos en la clase $A_r$ vendr\'a dada por una distribuci\'on multinomial:
\[p(n_1,\ldots,n_r)=\dfrac{n!}{n_1!\cdots n_r!}p_1^{n_1}\cdots p_r^{n_r}\]

Y cada $n_i$ sigue una distribuci\'on marginal binomial, por lo que su valor esperado ser\'a, $E(n_i)=np_i=E_i$. Por tanto tendremos que medir si la diferencia entre este valor esperado y las frecuencias observadas en nuestra muestra es significativa como para rechazar la hip\'otesis nula.

Como medida de esta diferencia, Pearson propuso el siguiente estad\'istico:
\[X^2=\sum_{i=1}^r\dfrac{(n_i-E_i)^2}{E_i}=\sum_{i=1}^r\dfrac{(n_i-np_i)^2}{np_i}\]

llamado estad\'istico de bondad del ajuste.

La distribuci\'on conjunta multinomial de las $n_i$ puede obtenerse mediante la distribuci\'on  condicional de $r$ variables independientes con distribuci\'on de Poisson con par\'ametro $np_i$, donde $p_1+\cdots+p_r=1$. Por tanto, las variables tipificadas:
\[\xi_i=\dfrac{n_i-np_i}{\sqrt{np_i}}\]

Son asint\'oticamente normales, $N(0;1)$ e independientes entre s\'i. Adem\'as, el hecho de que $\sum_{i=1}^rn_i=n$ implica que $\sum_{i=1}^r\sqrt{p_i}\xi_i=0$, lo que conduce a que:
\[X^2=\sum_{i=1}^r\dfrac{(n_i-np_i)^2}{np_i}=\sum_{i=1}^r\xi_i^2\]

Que ser\'a una distribuci\'on $\chi^2$ con $r-1$ grados de libertad, ya que la restricci\'on $\sum_{i=1}^r\sqrt{p_i}\xi_i=0$ elimina un grado de libertad.

Por tanto,
\[X^2=\sum_{i=1}^r\dfrac{(n_i-E_i)^2}{E_i}=\sum_{i=1}^r\dfrac{(n_i-np_i)^2}{np_i}\sim\chi^2_{r-1}\]

Valores elevados del estad\'istico implican una diferencia significativa entre los datos esperados seg\'un la hip\'otesis nula y los datos obtenidos en la muestra, por tanto la regi\'on cr\'itica ser\'a de la forma $C=\{X^2\geq K\}$, siendo $K$ aquel valor que, para un nivel de significaci\'on $\alpha$, cumpla:
\[P(X^2\geq K|H_0)=\alpha\]

Y, como bajo $H_0$ la distribuci\'on de $X^2$ es una $\chi^2_{k-1}$, podemos ontener el valor de $K$ de las tablas de la distribuci\'on $\chi^2$.

Hay que tener en cuenta que los $\xi_i$ siguen una distribuci\'on $N(0;1)$ de forma asint\'otica, y por tanto la distribuci\'on del estad\'istico tambi\'en ser\'a asint\'otica. Para que esta aproximaci\'on sea aceptable se suele exigir que el valor esperado de las $n_i$ sea mayor o igual que cinco para todas las categor\'ias en que se divide el espacio de variaci\'on. Si esta condici\'on no se cumple, habr\'a que agrupar una o varias categor\'ias hasta que se de la condici\'on.

Para que la aproximaci\'on sea de mayor calidad, tabi\'en es aconsejable que las categor\'ias se construyan de manera que las probabilidades sean aproximadamente iguales.

Una de las mayores ventajas de este contraste es que se puede aplicar a distribuciones discretas y continuas. Adem\'as, las categor\'ias se pueden asignar a conjuntos de valores de un atributo que no tiene por que ser num\'erico, ni siquiera ordinal.

\subsectioncol{Hip\'otesis nula compuesta.}

En este caso, la hip\'otesis nula especifica la familia de distribuciones a la que pertenece la poblaci\'on, pero no define el valor de los $k$ par\'ametros desconocidos que caracterizan esa familia. Por tanto, las probabilidades de cada familia depender\'an de esos par\'ametros.

Fischer demostr\'o que si se estiman los par\'ametros con la misma informaci\'on muestral con la que se realiza el contraste, el estad\'stico:
\[X^2=\sum_{i=1}^r\dfrac{(n_i-\hat{E}_i)^2}{\hat{E}_i}=\sum_{i=1}^r\dfrac{[n_i-np_i(\theta_1,\ldots,\theta_k)]^2}{np_i(\theta_1,\ldots,\theta_k)}\]

Tiene como ditribuci\'on asint\'otiva una $\chi^2$ con $r-k-1$ grados de libertad, siendo $k$ el n\'umero de par\'ametros que es necesario estimar. la forma de la regi\'on cr\'itica es an\'aloga a la de la hip\'otesis simple.

\sectioncol{Contraste de Kolmogorov-Smirnof.}

La metodolog\'ia del contraste $\chi^2$ discretiza las observaciones de la muestra. Esto, para distribuciones cont\'inuas en las que el tama\~no de la muestra no sea suficiente para realizar una partici\'on fina del recorrido de la variable provoca que no se utilice de forma eficiente la informaciÂº'on, y que la aproximaci\'on a la distribuci\'on del estad\'istico no sea muy buena.

El contraste de Kolmogorov-Smirnoff es m\'as eficiente para casos en los que la distribuci\'on de la hip\'otesis nula sea continua. Se basa en comparar la funci\'on de distribuci\'on emp\'irica con la funci\'on de distribuci\'on te\'orica. Definimos la funci\'on de distribuci\'on emp\'irica de la muestra como:
\[F_n(x)=\dfrac{\sum_{i=1}^nI_{(-\infty,x]}(x_i))}{n}\]

donde $I_{(-\infty,x)}(x_i)$ es la funci\'on indicador, que cumple que $I_{(-\infty,x]}(x_i)=1$ si $x_i\in(-\infty,x]$ y $I_{(-\infty,x]}(x_i)=0$ si $x_i\notin(-\infty,x]$.

Si la poblaci\'on de la que viene la muestra tiene una funci\'on de distribuci\'on $F$, el teorema de Glvenko-Cantelli nos dice que hay una probabilidad uno de que al aumentar $n$ $F_n$ converja a $F$, es decir, $F_n\overset{c.s.}{\to}F$, o lo que es lo mismo, sea $\Delta_n=\sup_{x\in\mathbb{R}}|F_n(x)-F(x)|$,
\[P\left[\lim_{n\to\infty}\Delta_n=0\right]=1\]

Por tanto, para contrastar la hip\'otesis nula $H_0:F=F_0$, se utiliza el estad\'istico:
\[\Delta_n=\sup_{x\in\mathbb{R}}|F_n(x)-F_0(x)|\]

Como cuanto mayor sea el estad\'istico mayor ser\'a la discrepancia entre la funci\'on de distribuci\'on emp\'irica y la te\'orica, la regi\'on cr\'itica es de la forma $C=\{\Delta_n>c\}$.

Se sabe que si $F_0$ es cont\'inua la distribuci\'on del estad\'istico no depende de $F_0$. Esto ha permitido tabular la distribuci\'on para valores peque\~nos de $n$ (debido a Massey). Para valores grandes, se utiliza la distribuci\'on asint\'otica probada por Kolmogorov y Smirnof:

\[P\left(\sqrt{n}\Delta_n\leq z\right)\to\sum_{k=-\infty}^{\infty}(-1)^ke^{2k^2z^2}\]

que tambi\'en est\'a tabulada.
